[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Programming with R",
    "section": "",
    "text": "This course requires no prior experience in programming. Yet, if you have some programming experience (e.g. SPSS, Stata, HTML), it will be helpful. R is an interpreted languages. In other words, the programs do not need compilation but will run in an environment to get the outputs. In this course, that is RStudio.\nAll packages and accounts are free and supported by open sources. It is recommended students bring their own computers (not mobile device) running MacOS, Linux or Windows operating systems.\nRecommended software and IDE’s:\n\nR version 4.2.1 or later (https://cran.r-project.org)\nRStudio version 1.2.x (https://www.rstudio.com)\nText editor of own choice (e.g. Atom, Sublime Text, Ultraedit)\n\nRecommended websites/accounts:\n1. GitHub (https://github.com)\n2. RStudio Cloud"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "Data collection\nData visualization\nData management\nData modeling\n\nThe major platform is R with the IDE (Integrated Development Environment) is Rstudio. However, the principles and applications can be used for other languages and platforms such as Python and Julia."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "This chapter introduces the general principles for data programming or coding involving data. Data programming is a practice that works and evolves with data. Unlike the point-and-click approach, programming allows the user to manage most closely the data and process data in more effective manner. Programs are designed to be replicable, by user and collaborators. A data program can be developed and updated iteratively and incrementally. In other words, it is building on the culminated works without repeating the steps. It takes debugging, which is the process of identifying problems (bugs) but, in fact, updating the program in different situations or with different inputs when used in different contexts, including the programmer himself or herself working in future times."
  },
  {
    "objectID": "intro.html#principles-of-data-programming",
    "href": "intro.html#principles-of-data-programming",
    "title": "2  Introduction",
    "section": "2.1 Principles of Data Programming",
    "text": "2.1 Principles of Data Programming\nSocial scientists Gentzkow and Shapiro (2014) list out some principles for data programming.\n\nAutomation\n\nFor replicability (future-proof, for the future you)\n\n\n\n\nVersion Control\n\nAllow evolution and updated edition\nUse Git and GitHub\n\n\n\n\nDirectories/Modularity\n\nOrganize by functions and data chunks\n\n\n\n\nKeys\n\nIndex variable (relational)\n\n\n\n\nAbstraction\n\nKISS (Keep in short and simple)\n\n\n\n\nDocumentation\n\nComments for communicating to later users\n\n\n\n\nManagement\n\nCollaboration ready"
  },
  {
    "objectID": "intro.html#functionalities-of-data-programs",
    "href": "intro.html#functionalities-of-data-programs",
    "title": "2  Introduction",
    "section": "2.2 Functionalities of Data Programs",
    "text": "2.2 Functionalities of Data Programs\nA data program can provide or perform :\n\nData source\nDocumentation of data\nImporting and exporting data\nManagement of data\nVisualization of data\nData models\n\nSample R Programs:\nR basics\n\n# Create variables composed of random numbers\nx <-rnorm(50) \ny = rnorm(x)\n\n# Plot the points in the plane \nplot(x, y)\n\n\n\n\nUsing R packages\n\n# Plot better, using the ggplot2 package \n## Prerequisite: install and load the ggplot2 package\n## install.packages(\"ggplot2\")\nlibrary(ggplot2)\nqplot(x,y)\n\n\n\n\nMore R Data Visualization\n\n# Plot better better with ggplot2\nlibrary(ggplot2)\nggplot(,aes(x,y)) + theme_bw() + geom_point(col=\"blue\")"
  },
  {
    "objectID": "Rbasics.html",
    "href": "Rbasics.html",
    "title": "3  R Basic operations",
    "section": "",
    "text": "This chapter introduces basic R operations including installing packages, operators, loading libraries and writing functions."
  },
  {
    "objectID": "Rbasics.html#create-a-project",
    "href": "Rbasics.html#create-a-project",
    "title": "3  R Basic operations",
    "section": "3.1 Create a project",
    "text": "3.1 Create a project\nBefore diving into R programming, it is highly recommended creating a project. It will organize all related objects and files under one roof. Version control can be applied for automatically saving history (log) and data objects. Projects can facilitate additional specialized works such as:\n\nWriting a book using Quarto or bookdown\nCreating a website or a blog using Quarto\n\nHighly recommended to use this feature to create personal website\n\nSoftware development\nData visualization using Shiny\nManage data in database servers or other platform\n\nHowever, to start R programming with a project helps better organize the process of data analytics. A program is not just running a procedure but performing more data science tasks that last more than just one time execution. It scales!\nFor more detail, consult Using RStudio Projects"
  },
  {
    "objectID": "Rbasics.html#operators",
    "href": "Rbasics.html#operators",
    "title": "3  R Basic operations",
    "section": "3.2 Operators",
    "text": "3.2 Operators\n\n3.2.1 Assignment operators\n<- and = are both the assignment operator where as = must be used as top level.\n\n\n\n|> is the base R “pipe” operator, feeding value (arguement) into a function.\n\n\n[1] 15\n\n\nThe other pipe operator %>% (package magrittr), which is more commonly used especially in tidyverse. Using %>%, the argument can be piped into the function without ().\n\n\n[1] 15\n\n\n\n\n3.2.2 Math operators\nThe common operators used in math are also applicable in the R environment.\n\nArithmetic: +,-,*,/,^ (power)\nLogical: & (and), | (or), ! (Not)\nRelational: >,<, ==, >=, <=, !="
  },
  {
    "objectID": "Rbasics.html#package-management",
    "href": "Rbasics.html#package-management",
    "title": "3  R Basic operations",
    "section": "3.3 Package Management",
    "text": "3.3 Package Management\n\n3.3.1 Install packages\nR comes with basic functions and demo datasets (e.g. mtcars, Titanic, iris). For additional functionalities or specialized functions, installing additional packages is needed. Use install.packages(\"ISLR2\") for installation and it only needs be done the first time. However, for every new session, library() function is needed to call in (load) the package for remaining program.\n\n\n3.3.2 Libraries\nThe library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. The \\(ISLR\\) book uses the MASS package, which is a very large collection of data sets and functions. The ISLR2 package also includes the data sets associated with this book for demonstration.\n\nlibrary(MASS)\nlibrary(ISLR2)\n\n\nAttaching package: 'ISLR2'\n\n\nThe following object is masked from 'package:MASS':\n\n    Boston\n\n\nIf you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR2, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(\"ISLR2\"). This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session.\n\n\n3.3.3 Speed and organization\nThe general principle of using packages is: only load what you need! There are over 20,000 packages but the memory is limited. This article gives some comparison on different methods of using and loading packages or libraries. Again, using Project to manage your resources and it is advised to restart the R session (Session –> Restart R) to start with a clean slate for each project.\n\n\n3.3.4 Managing packages and Library\nThere are multiple ways of managing packages in each R session. Following the general principle of keeping a lean model, it is recommended to only load packages in library for each data program. Some useful hotkeys and functions:\nSome useful hotkeys:\n\nRestarting R: Ctrl+Shift+F10 (Mac: Command+Shift+0) = unload packages + clear objects (provided that in Tools/ Global options in Windows (Preference in Mac), Save workspace to .RData option is unchecked and set to never)\nuse gc() free up memory and report the memory usage.\nrm(list=ls()) clear objects manually\n\n\ngc()\n\n          used (Mb) gc trigger (Mb) limit (Mb) max used (Mb)\nNcells  603436 32.3    1350921 72.2         NA  1350921 72.2\nVcells 1114950  8.6    8388608 64.0      16384  1839818 14.1\n\n\n\nrows “Ncells” (cons cells): 28 bytes each on 32-bit systems and 56 bytes on 64-bit systems).\ncolumns Vcells: allocated space for vectors in multiples of 8 bytes\nused column reports how many cells have been allocated and the following column reports the total size in Megabytes.\ngc trigger column reports when the next garbage collection will occur.\nmax used reports the maximum memory that was used since the last call to the gc() function."
  },
  {
    "objectID": "Rbasics.html#writing-functions",
    "href": "Rbasics.html#writing-functions",
    "title": "3  R Basic operations",
    "section": "3.4 Writing Functions",
    "text": "3.4 Writing Functions\nAs we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide examples of simple functions. The first one reads in the ISLR2 and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.\n\nLoadLibraries\n\nError in eval(expr, envir, enclos): object 'LoadLibraries' not found\n\nLoadLibraries()\n\nError in LoadLibraries(): could not find function \"LoadLibraries\"\n\n\nWe now create the function. Note that the + symbols are printed by R and should not be typed in. The { symbol informs R that multiple commands are about to be input. Hitting Enter after typing { will cause R to print the + symbol. We can then input as many commands as we wish, hitting {Enter} after each one. Finally the } symbol informs R that no further commands will be entered.\n\nLoadLibraries <- function() {\n  library(ISLR2)\n  library(MASS)\n  print(\"The libraries have been loaded.\")\n}\n\nNow if we type in LoadLibraries, R will tell us what is in the function.\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\"\n\n\nIf we call the function, the libraries are loaded in and the print statement is output.\nThe following example demonstrates creating a function out of existing functions from different packages:\n\n###Writing R functions\n## Combine the lm, plot and abline functions to create a one step regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y)\n  abline(fit,col=\"red\")\n}\nattach(Carseats)\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}  # \"...\" is called ellipsis, which is designed to take any number of named or unnamed arguments.\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n\n\n\nThe following example creates a function to deal with package management:\n\n# Create preload function\n# Check if a package is installed.\n# If yes, load the library\n# If no, install package and load the library\n\npreload<-function(x)\n{\n  x <- as.character(x)\n  if (!require(x,character.only=TRUE))\n  {\n    install.packages(pkgs=x,  repos=\"http://cran.r-project.org\")\n    require(x,character.only=TRUE)\n  }\n}\n\nLet’s try preloading the package name **tidyverse** (be sure to wrap the name with double quotes ““:"
  },
  {
    "objectID": "Rbasics.html#workshop-1",
    "href": "Rbasics.html#workshop-1",
    "title": "3  R Basic operations",
    "section": "3.5 Workshop 1",
    "text": "3.5 Workshop 1\n\nDownload and open packagemanagement.R from class GitHub under codes\nTry each method one at a time\nWhich one is more convenient to you?"
  },
  {
    "objectID": "Quarto.html",
    "href": "Quarto.html",
    "title": "4  Quarto",
    "section": "",
    "text": "This chapter gives a brief introduction of communicative programming using Quarto. The idea is to incorporate data programming elements into a publishable document. Formats including PDF and HTML.\nQuarto is an open-source scientific and technical publishing system built on a document converter called Pandoc. It is multilingual (R, Python, Julia and Observable) and can be used for creating a wide variety of documents using simple plain text markdown (e.g. R Markdown). Examples include website (static), academic reports, presentations, blogs. It can accommodate LaTeX for scientific equations and more advanced features such as blogs and conversion to pdf, html and MS Word.\nThink of Quarto a platform with programming tools using R Markdown for:"
  },
  {
    "objectID": "Datacollection.html",
    "href": "Datacollection.html",
    "title": "5  Data Collection with R",
    "section": "",
    "text": "5.0.1 What is Big Data?\nThe [term] Big Data is about data that has huge volume, cannot be on one computer. Has a lot of variety in data types, locations, formats and form. It is also getting created very very fast (velocity) (Doug Laney 2001).\nBig data is about the size of the data file. Big data is about the data generating process and data collection process.\nSurvey data, no matter how many cases, should be classified as small data instead of big data due to the collection process and design.\nBurt Monroe, the founder of the Social Data Analytics program at the Penn State University, which is the first of its kind, gave new V’s to Big Data.\n\nVolume\nVariety\nVelocity\nVinculation\nValidity\n\nHe argues that Big Data is not just big, diverse and fast, it is inter-connected and must search for significance.\n\n\n5.0.2 Why we need to collect data or original data?\nOne key element of data programming is the program is created for data, including collecting and updating data. Collection or production of data constitutes the major component in data science. No data scientist can count on others to provide data without knowing the source and generation method of the data. Knowing the data generation process is critical in preparing data for the rest of the data science processes including management, visualization and in particular modeling. Imagine if the date generation process is unknown, visualization and model thus created could be faulty. Missing values, for instance, can mask the visualization and totally distort the modeling results.\n\n\n5.0.3 Types of data by data generation\n\nMade data vs. Found data\nStructured vs. Semi/unstructured\nPrimary vs. secondary data\nDerived data\nmetadata, paradata\n\n\n5.0.3.1 Made data (by production)\n\nSurvey\nInterviews\nExperiments\nFocus group\n\n\n\n5.0.3.2 Found data (by collection)\n\nOpen data\nAPI\nNon-API\nOpen data\n\nOpen data refers to the type of data usually offered by government (e.g. Census), organization or research institutions (e.g. ICPSR. Some may require application for access and others may be open for free access (usually via websites or GitHub).\nSince open data are provided by government agencies or research institutions, these data files are often:\n\nStructured\nWell documented\nReady for data/research functions\n\n\nAPI\n\nAPI stands for Application Programming Interface. It is a web service that allows an interaction with, and retrieval of, structured data from a company, organization or government agency.\nExample: Social media (e.g. Facebook, YouTube, Twitter), Government agency (e.g. Congress)\nAPIs can take many different forms and be of varying quality and usefulness. RESTful API (Representational State Transfer) is a means of transferring data using web protocols\nLike open data, data available through API are generally:\n\nStructured\nSomewhat documented\nNot necessary fully open\nSubject to the discretion of data providers (e.g. Not all variables are available, rules may change without announcements, etc.)\n\nExample:\n\nCrossref API\nTaiwan Legislative Yuan (Congress) API\n\n\nNon-API\n\nFor the type of found data not available via API or open access, one can use non-API methods to collect this kind of data. These methods include scraping, which is to simulate web browsing but through automated scrolling and parsing to collect data. These data are usually non-structured and often times noisy. Researchers also have little control over data generation process and sampling design.\nNon-API data are generally:\n\nNon-structured\nNoisy\nUndocumented with no or little information on sampling\n\n\n\n\n5.0.4 Illustrations: Open data\n\n5.0.4.1 Stock data\n\n# Collecting Stock data and plotting stock data as time series\n# install.packages(c(\"quantmod\", \"ggplot2\", \"magrittr\",\"broom\",\"googlesheet4\"))\n# lapply(c(\"quantmod\", \"ggplot2\", \"magrittr\",\"broom\",\"googlesheet4\"), require, character.only = TRUE)\nlibrary(quantmod)\nlibrary(ggplot2)\nlibrary(magrittr)\nlibrary(broom)\n\n# Setting time period\nstart = as.Date(\"2010-07-01\") \nend = as.Date(\"2022-09-30\")\ngetSymbols(\"AAPL\")\n\n[1] \"AAPL\"\n\nchartSeries(AAPL, theme=\"white\")\n\n\n\n\n\n# Download Taiwan Weighted Index\ngetSymbols(\"^TWII\", src=\"yahoo\") # TWSE:IND\n\n[1] \"^TWII\"\n\n# Download Dow Jones Industrial Average\ngetSymbols(\"^DJI\", src=\"yahoo\") # Dow Jones Industrial Average\n\n[1] \"^DJI\"\n\n# Simple plot\nplot(TWII, col=\"darkblue\")\n\n\n\n# Plot candle stick and other charts using quantmod\nchartSeries(DJI)\n\n\n\nchartSeries(DJI, type = c(\"auto\", \"candlesticks\", \"matchsticks\", \"bars\",\"line\"), subset='last 4 months',theme = \"white\")\n\n\n\nbarChart(DJI,multi.col=TRUE,theme = 'white')\n\n\n\nlineChart(DJI,line.type = 'l', theme = 'white') # line, choices include l, h, c, b\n\n\n\nlineChart(DJI,line.type = 'h',theme = chartTheme('white', up.col='steelblue')) # histogram\n\n\n\ncandleChart(DJI,subset = '2020-01/2022-01', multi.col=TRUE,theme = chartTheme('white'))\n\n\n\n## grey => Open[t] < Close[t] and Op[t] < Cl[t-1]\n## white => Op[t] < Cl[t] and Op[t] > Cl[t-1]\n## red => Op[t] > Cl[t] and Op[t] < Cl[t-1]\n## black => Op[t] > Cl[t] and Op[t] > Cl[t-1]\n\n\n## Plotting multiple series using ggplot2\n# Collect stock names from Yahoo Finance\ngetSymbols(c(\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"GOOGL\"), src = \"yahoo\", from = start, to = end)\n\n[1] \"AAPL\"  \"MSFT\"  \"AMZN\"  \"TSLA\"  \"GOOGL\"\n\n# Prepare data as xts (time series object)\nstocks = as.xts(data.frame(AAPL = AAPL[, \"AAPL.Adjusted\"], \n                           MSFT = MSFT[, \"MSFT.Adjusted\"], \n                           AMZN = AMZN[, \"AMZN.Adjusted\"],\n                           GOOGL = GOOGL[, \"GOOGL.Adjusted\"],\n                           TSLA = TSLA[, \"TSLA.Adjusted\"]))\n\n# Index by date\nnames(stocks) = c(\"Apple\", \"Microsoft\", \"Amazon\", \"Google\", \"Tesla\")\nindex(stocks) = as.Date(index(stocks))\n\n# Plot\nstocks_series = tidy(stocks) %>% \n  ggplot(aes(x=index,y=value, color=series)) + \n  geom_line(cex=1) +\n  theme_bw()\nstocks_series\n\n\n\n# Plot TWII\nTWII_series = tidy(TWII$TWII.Adjusted) %>% \n  ggplot(aes(x=index,y=value, color=series)) + \n  geom_line(cex=1) +\n  theme_bw()\nTWII_series\n\n\n\nstocks_series = tidy(stocks) %>% \n  ggplot(aes(x=index,y=value, color=series)) + \n  geom_line(cex=1) +\n  theme_bw() +\n  labs(title = \"Daily Stock Prices, 7/1/2010 - 11/16/2021\",\n     subtitle = \"End of Day Adjusted Prices\",\n     caption = \"Source: Yahoo Finance\") +\n  xlab(\"Date\") + ylab(\"Price\") +\n  scale_color_manual(values = c(\"steelblue\", \"firebrick\",\"purple\", \"forestgreen\",\"darkgray\")) +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) +\n  theme(legend.position=\"top\")\nstocks_series\n\n\n\n\n\n# Fetching stock data using Googlesheet\nlibrary(googlesheets4)\nlibrary(httpuv)\n\n# Read stock data using Googlesheet with public link\n# Needs authentication of Google account via browser (when prompted)\n# aapl = read_sheet(\"https://docs.google.com/spreadsheets/d/1vTdXZkqpIZwUsnxM8zXiXVyKI7BbZ9zdPNEV_WLWDXo/edit?usp=sharing\")\n\n\n\n5.0.4.2 COVID\nMany organizations put in tremendous efforts to collect, visualize and disseminate COVID data to facilitate research of the pandemic. The most well-known is the Johns-Hopkins Coronavirus Resource Center. Collecting data from all over the world data, these data are stored in the Center’s GitHub ready for any researchers to get access. Other widely used data GitHub’s includng New York Times and Our World in Data (OWID)\nThe following illustration uses the OWID data:\n\n# Illustration: collecting open COVID data from OWID GitHub\n## Packages used: vroom, tidyverse, finalfit\n## Use install.packages() function to install these packages.\n\nlibrary(vroom) # Fast reading in data\nlibrary(finalfit) #for checking missingness and output visualization\nlibrary(tidyverse)\nlibrary(RColorBrewer) #for choice of more colors\nlibrary(scales) # for controlling scales for x and y axes\n\n\n# Reading all real time data directly from OWID GitHub\n# vroom is the champion in reading github date, < 3 sec.\nowidall = vroom(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\n\n\nas.factor(owidall[,1:3]) # Redefining first 3 columns as factor (categorical variables)\n\n iso_code continent  location \n     <NA>      <NA>      <NA> \n3 Levels: c(\"AFG\", \"OWID_AFR\", \"ALB\", \"DZA\", \"AND\", \"AGO\", \"AIA\", \"ATG\", \"ARG\", \"ARM\", \"ABW\", \"OWID_ASI\", \"AUS\", \"AUT\", \"AZE\", \"BHS\", \"BHR\", \"BGD\", \"BRB\", \"BLR\", \"BEL\", \"BLZ\", \"BEN\", \"BMU\", \"BTN\", \"BOL\", \"BES\", \"BIH\", \"BWA\", \"BRA\", \"VGB\", \"BRN\", \"BGR\", \"BFA\", \"BDI\", \"KHM\", \"CMR\", \"CAN\", \"CPV\", \"CYM\", \"CAF\", \"TCD\", \"CHL\", \"CHN\", \"COL\", \"COM\", \"COG\", \"COK\", \"CRI\", \"CIV\", \"HRV\", \"CUB\", \"CUW\", \"CYP\", \"CZE\", \"COD\", \"DNK\", \"DJI\", \"DMA\", \"DOM\", \"ECU\", \"EGY\", \"SLV\", \"GNQ\", \"ERI\", \"EST\", \"SWZ\", \"ETH\", \"OWID_EUR\", \"OWID_EUN\", \\n\"FRO\", \"FLK\", \"FJI\", \"FIN\", \"FRA\", \"PYF\", \"GAB\", \"GMB\", \"GEO\", \"DEU\", \"GHA\", \"GIB\", \"GRC\", \"GRL\", \"GRD\", \"GUM\", \"GTM\", \"GGY\", \"GIN\", \"GNB\", \"GUY\", \"HTI\", \"OWID_HIC\", \"HND\", \"HKG\", \"HUN\", \"ISL\", \"IND\", \"IDN\", \"OWID_INT\", \"IRN\", \"IRQ\", \"IRL\", \"IMN\", \"ISR\", \"ITA\", \"JAM\", \"JPN\", \"JEY\", \"JOR\", \"KAZ\", \"KEN\", \"KIR\", \"OWID_KOS\", \"KWT\", \"KGZ\", \"LAO\", \"LVA\", \"LBN\", \"LSO\", \"LBR\", \"LBY\", \"LIE\", \"LTU\", \"OWID_LIC\", \"OWID_LMC\", \"LUX\", \"MAC\", \"MDG\", \"MWI\", \"MYS\", \"MDV\", \"MLI\", \"MLT\", \"MHL\", \"MRT\", \"MUS\", \"MEX\", \\n\"FSM\", \"MDA\", \"MCO\", \"MNG\", \"MNE\", \"MSR\", \"MAR\", \"MOZ\", \"MMR\", \"NAM\", \"NRU\", \"NPL\", \"NLD\", \"NCL\", \"NZL\", \"NIC\", \"NER\", \"NGA\", \"NIU\", \"OWID_NAM\", \"PRK\", \"MKD\", \"OWID_CYN\", \"MNP\", \"NOR\", \"OWID_OCE\", \"OMN\", \"PAK\", \"PLW\", \"PSE\", \"PAN\", \"PNG\", \"PRY\", \"PER\", \"PHL\", \"PCN\", \"POL\", \"PRT\", \"PRI\", \"QAT\", \"ROU\", \"RUS\", \"RWA\", \"SHN\", \"KNA\", \"LCA\", \"SPM\", \"VCT\", \"WSM\", \"SMR\", \"STP\", \"SAU\", \"SEN\", \"SRB\", \"SYC\", \"SLE\", \"SGP\", \"SXM\", \"SVK\", \"SVN\", \"SLB\", \"SOM\", \"ZAF\", \"OWID_SAM\", \"KOR\", \"SSD\", \"ESP\", \"LKA\", \"SDN\", \\n\"SUR\", \"SWE\", \"CHE\", \"SYR\", \"TWN\", \"TJK\", \"TZA\", \"THA\", \"TLS\", \"TGO\", \"TKL\", \"TON\", \"TTO\", \"TUN\", \"TUR\", \"TKM\", \"TCA\", \"TUV\", \"UGA\", \"UKR\", \"ARE\", \"GBR\", \"USA\", \"VIR\", \"OWID_UMC\", \"URY\", \"UZB\", \"VUT\", \"VAT\", \"VEN\", \"VNM\", \"WLF\", \"ESH\", \"OWID_WRL\", \"YEM\", \"ZMB\", \"ZWE\") ...\n\n# Subset by year\n\nowid2022 = subset(owidall, format(as.Date(date),\"%Y\")==2022)\nowid2021 = subset(owidall, format(as.Date(date),\"%Y\")==2021)\nowid2020 = subset(vroom(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\"), format(as.Date(date),\"%Y\")==2020) # Direct from source\n\n# Clean up OWID*  cases\n# Deselect cases/rows with OWID\nowidall = owidall[!grepl(\"^OWID\", owidall$iso_code), ] \nowid2022 = owid2022[!grepl(\"^OWID\", owid2022$iso_code), ] \nowid2021 = owid2021[!grepl(\"^OWID\", owid2021$iso_code), ]\nowid2020 = owid2020[!grepl(\"^OWID\", owid2020$iso_code), ]\nowidall$location=as.factor(owidall$location)\n\n# Subset by country: United States\nowidus = subset(owidall, location==\"United States\")\nowideu = subset(owidall, continent==\"Europe\")\nowidasia = subset(owidall, continent==\"Asia\")\n\n\n# Get today's COVID data\nowidtoday = subset(vroom(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\"), date == Sys.Date())\n\nowid09302022 = subset(owidall,  date == \"2022-09-30\")\n\nowidUStoday = subset(owid2022, location == \"United States\" & date >=\"2022-09-30\")\nowidtw2022 = subset(owid2022, location == \"Taiwan\")\n\n\noptions(scipen=999)\npar(family = \"Palatino\")\n\n\n# Europe data\ny = owideu$new_deaths\nx = as.Date(owideu$date)\nplot(x,y, pch=20, col=\"#E7298A\", cex = .5, xaxt='n', xlab = \"Date\", ylab = \"COVID Deaths in Europe (Daily)\")\naxis(1, x, format(x, \"%Y-%m\"), cex.axis = .7, las = 3 , gap.axis =1.5, tick = FALSE)\n\n\n\n# identify(x,y,owideu$location, ps=8, atpen=TRUE) \n# Use identify function to manually click on cases using mouse\n\n\n# Asia data\ny = owidasia$new_deaths\nx = as.Date(owidasia$date)\nplot(x,y, pch=20, col=\"#66A61E\",  cex = .5, xaxt='n', xlab = \"Date\", ylab = \"COVID Deaths in Asia (Daily)\")\naxis(1, x, format(x, \"%Y-%m\"), cex.axis = .7, las = 3 , gap.axis =1.5, tick = FALSE)\n\n\n\n# identify(x,y,owidasia$location, ps=8, atpen=TRUE)\n\n# Format date\nowidall$date<-as.Date(owidall$date,format=\"%Y-%m-%d\")\n# World: new cases\n\nplot(owidall$date,owidall$new_cases, pch = 20, cex = .3, xaxt='n')\npoints(owidtw2022$date,owidtw2022$total_tests, pch = 20, cex = .5, xaxt='n', col = \"firebrick\")\naxis(1, owidall$date, format(owidall$date, \"%Y-%m-%d\"), cex.axis = .3, las = 3 )\n\n\n\nlibrary(descr)\nfreq(owidall$continent)\n\n\n\n\nowidall$continent \n              Frequency Percent\nAfrica            50858  24.665\nAsia              46919  22.754\nEurope            46675  22.636\nNorth America     33864  16.423\nOceania           15661   7.595\nSouth America     12221   5.927\nTotal            206198 100.000\n\noptions(scipen=999) # No sci notation\n# format(new_cases, scientific = F)"
  },
  {
    "objectID": "Datamanagement.html",
    "href": "Datamanagement.html",
    "title": "6  Data Management with R",
    "section": "",
    "text": "Spark with R\nApache Spark is a distributed computing platform empowered to run analytics at a large scale and with sizable big data (Luraschi, Kuo, Ruiz 2020)\nThe concept of distributed computing is building on Google’s MapReduce: map and reduce. The map operation provides an arbitrary way to transform each file into a new file, whereas the reduce operation combines two files. Both operations require custom computer code, but the MapReduce framework takes care of automatically executing them across many computers at once. These two operations are sufficient to process all the data available on the web, while also providing enough flexibility to extract meaningful information from it. (LKR, Chapter 1)\nSpark provided a richer set of verbs beyond MapReduce to facilitate optimizing code running in multiple machines. Spark also loaded data in-memory, making operations much faster than Hadoop’s on-disk storage.\nThis chapter introduces using Spark with R, which is particularly designed for high power data modeling with big data.\nTo get started, it is recommended to install Spark on your local machine (RStudio Cloud is not supporting Spark yet).\nThe following demonstrates the installation of a local Spark and running simple modeling procedures in modeling Taiwan election data.\n\ninstall.packages(\"sparklyr\")\nlibrary(sparklyr)\n\n# install Spark on your local computer, treating it like a cluster\n# internet connection required\n\nspark_install()\n\n# Connect to cluster\nsc <- spark_connect(master = \"local\")\n\n# Check Spark connections and environment\n\n# Copy data to Spark session's memory\ntbl_teds16 <- copy_to(sc, TEDS2016, \"spark_teds2016\")\nclass(tbl_teds16)\n\n# Alternative method to load local csv to Spark\nspark_read_csv(sc, name = \"teds16\",  path = \"/path/TEDS2016.csv\")\n\n# Disconnect\nspark_disconnect(sc)\n\nsdf_describe(votetsai, cols = colnames(votetsai))\n\npartitions <- tbl_teds16 %>%\n  select(votetsai, dpp, kmt, unify, statusquo, female) %>% \n  sdf_random_split(training = 0.5, test = 0.5, seed = 1099)\n\n\nfit <- partitions$training %>%\n  ml_logistic_regression(votetsai ~ .)\nsummary(fit)\n\npred <- ml_predict(fit, partitions$test)\n\n## Use Spark and H2O\n### https://docs.h2o.ai/sparkling-water/3.3/latest-stable/doc/rsparkling.html\n\nif (\"package:h2o\" %in% search()) { detach(\"package:h2o\", unload=TRUE) }\nif (\"h2o\" %in% rownames(installed.packages())) { remove.packages(\"h2o\") }\n\n# Install packages H2O depends on\npkgs <- c(\"methods\", \"statmod\", \"stats\", \"graphics\", \"RCurl\", \"jsonlite\", \"tools\", \"utils\")\nfor (pkg in pkgs) {\n  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }\n}\ninstall.packages(\"h2o\", \"3.38.0.1\")  \n# Download, install, and initialize the RSparkling\ninstall.packages(\"rsparkling\", type = \"source\", repos = \"http://h2o-release.s3.amazonaws.com/sparkling-water/spark-3.3/3.38.0.1-1-3.3/R\")\n\nlibrary(rsparkling)\n\nsc <- spark_connect(master = \"local\", version = \"3.3.0\")\nlibrary(h2o)\n#install.packages(\"rsparkling\")\nlibrary(rsparkling)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nh2oConf <- H2OConf()\nhc <- H2OContext.getOrCreate(h2oConf)\nhc$openFlow()\nvote_h2o <- hc$asH2OFrame(tbl_teds16)\n\n\nvote_glm <- h2o.glm(x = c(\"dpp\", \"female\"),\n                      y = \"votetsai\",\n                      training_frame = vote_h2o,\n                      lambda_search = TRUE)\nvote_glm"
  },
  {
    "objectID": "Datavisualization.html",
    "href": "Datavisualization.html",
    "title": "7  Data Visualization with R",
    "section": "",
    "text": "Data visualization is to deliver a message from your data. It is like telling a story using the chart or data applications. Sometimes the data is huge or the story to too long to tell. Visualization provides an ability to comprehend huge amounts of data. The important information from more than a million measurements is immediately available.\nVisualization often enables problems with the data to become immediately apparent. A visualization commonly reveals things not only about the data itself but also about the way it is collected. With an appropriate visualization, errors and artifacts in the data often jump out at you. For this reason, visualizations can be invaluable in quality control.\nVisualization facilitates understanding of both large-scale and small-scale features of the data. It can be especially valuable in allowing the perception of patterns linking local features.\nVisualization facilitates hypothesis formation, inviting further inquiries into building a theory (Colin Ware 2012). It is exploratory data anlaysis (EDA) but can also provide the tools for hypothesis confirmation."
  },
  {
    "objectID": "Datavisualization.html#learn-to-read-data",
    "href": "Datavisualization.html#learn-to-read-data",
    "title": "7  Data Visualization with R",
    "section": "7.2 Learn to read data",
    "text": "7.2 Learn to read data\nEdward Tufte is one of the earliest data scientists emphasizing visual thinking. He postulates that one should first learn to read data, before moving on to visualize. He suggests training the visual thinking, then preparing the educated eyes. His newest book is titled SEEING WITH FRESH EYES: MEANING, SPACE, DATA, TRUTH, vividly testifying his philosophy of connecting the human perception with the data message.\n\n\n\nEdward Tufte\n\n\n\n\n\nSeeing with Fresh Eyes: Meaning, Space, Data, Truth\n\n\nFor Tufte, number one thing to learn about data visualization is to discard the default.\n“If you’re not doing something different, you’re not doing anything at all.” - Edward Tufte"
  },
  {
    "objectID": "Datavisualization.html#references",
    "href": "Datavisualization.html#references",
    "title": "7  Data Visualization with R",
    "section": "7.3 References:",
    "text": "7.3 References:\nGraham Williams 2011. Data Mining with Rattle and R: The Art of Excavating Data for Knowledge"
  },
  {
    "objectID": "Datamodeling.html",
    "href": "Datamodeling.html",
    "title": "8  Data Modeling with R",
    "section": "",
    "text": "This chapter introduces methods of modeling data using R. Given the breadth of this topic, we will cover some basic machine learning models including:"
  },
  {
    "objectID": "whatsnext.html",
    "href": "whatsnext.html",
    "title": "9  What’s next",
    "section": "",
    "text": "This book will continue to incorporate new materials including new data science topics and developments."
  },
  {
    "objectID": "Rbasics.html#workshop-2",
    "href": "Rbasics.html#workshop-2",
    "title": "3  R Basic operations",
    "section": "3.6 Workshop 2",
    "text": "3.6 Workshop 2\n\nCreate the following objects:\n\nx <-rnorm(30)\ny = rnorm(x)\n\nPlot:\n\nhistogram of y (hint: use the hist() function)\nBoth x and y, using pch=20 (choose your own color using col=““)\n\nCheck the environment\n\nClean all objects using the following command:\n\nrm(list=ls())\n\n\nAlternatively, you can use the hotkeys to restart R session\nTry preload function to install and load packages\n\n\n3.6.1 Recommended R Resources:\n\nThe R Journal\nIntroduction to R by W. N. Venables, D. M. Smith and the R Core Team\nIntroduction to R Seminar at UCLA\nGetting Started in Data Analysis using Stata and R by Data and Statistical Services, Princeton University\n\n\n\n3.6.2 References:\nBryan, Jenny. STAT 545 (https://stat545.com) Bryan, Jenny and Jim Hester. What They Forgot to Teach You About R"
  },
  {
    "objectID": "Quarto.html#website",
    "href": "Quarto.html#website",
    "title": "4  Quarto",
    "section": "4.1 Website",
    "text": "4.1 Website\nThis session is extracted from this online notebook Building Personal Website on GitHub.\nExample\n\nCreation of website files using Quarto\nCreate a New Project (top right Project link in RStudio)\nChoose New Directory and Quarto Website\nGive the name of directory “username.github.io” with your GitHub username (e.g. karlho.github.io)\nEditing website files using Quarto\n\nFirst edit the _quarto.yml then other qmd files, which are Markdown files.\nThe _quarto.yml controls the style of the website:\nproject:\n  type: website\n\nwebsite:\n  title: \"karlho.github.io\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n\neditor: visual\n\nIn the website section, rename the title to your name (e.g. Karl Ho). The navbar controls the top navigation bar starting from the left. You can add other pages such as research, personal, etc.\nThe format part control document output. In this case, it exports into html with the choice of theme, which is from Bootstrap 3. The cosmo theme is used here but choose your favorite theme from the Bootstrap website (e.g. united, sandpaper).\nThen, the index.qmd file. This file is the front page (will be converted to index.html), so it is recommended to use this to introduce yourself or the latest developments (e.g. presentations, projects, etc.). You can replace with the following sample text:\n---\ntitle: \"John Doe\"\n---\n\nI am a doctoral student studying the political economy of East Asia and US.  My research topic is on rent-seeking behaviors in last two decades\n\n### More Information\n\nInterests:\n\n  * Data Science \n  * Time series modeling\n\n### Contact me\n\n[johndoe@utdallas.edu](mailto:johndoe@utdallas.edu)\n\n[Website](https://johndoe.github.io)  \nThe rest will be just edit the pages (qmd) you want to post on your website (i.e. research.qmd, teach.qmd, etc)\n\n---\ntitle: \"About\"\n---\n\nMore about this website.\n\nOnce all files are ready, click on the Render button. The rendering process converts the .qmd files into html files under the _site folder.\nNow your website is ready locally. You can check the links and make sure they all work and that related files (e.g. CV) are in the same _site folder. The next part is to deploy to your host, which is GitHub."
  },
  {
    "objectID": "Quarto.html#blog",
    "href": "Quarto.html#blog",
    "title": "4  Quarto",
    "section": "4.2 Blog",
    "text": "4.2 Blog"
  },
  {
    "objectID": "Quarto.html#academic-paper",
    "href": "Quarto.html#academic-paper",
    "title": "4  Quarto",
    "section": "4.3 Academic paper",
    "text": "4.3 Academic paper"
  },
  {
    "objectID": "Quarto.html#presentation",
    "href": "Quarto.html#presentation",
    "title": "4  Quarto",
    "section": "4.4 Presentation",
    "text": "4.4 Presentation\nReferences:\nQuarto Website\nQuarto Blog\nQuarto Presentations\nQuarto Journal Articles"
  },
  {
    "objectID": "Datamodeling.html#what-is-machine-learning",
    "href": "Datamodeling.html#what-is-machine-learning",
    "title": "8  Data Modeling with R",
    "section": "8.1 What is machine learning?",
    "text": "8.1 What is machine learning?\n\n8.1.1 Statistics and Machine Learning\nLet’s startswith basic concepts of statistics and machine learning models. Traditionally, we use statistics to estimate models and test hypotheses using data. We can collect data, describe variables such as averages, variances and distributions and then explain relationships between two or more variables. The roots of statistics lie in working with data and checking theory against data (Breiman 2001). This data model replies heavily on the theory and hypotheses to first generate or collect data to test the model, in an attempt to explain the relationship before prediction. Machine learning takes on a different approach. It focuses on prediction by using algorithmic models without first developing a theory then hypothesis. UC Berkeley Statistics professor Leo Breiman compares the two cultures of statistics in his famous 2001 paper and explains the difference. He argues that with so much data coming in from all sources and directions, using the data model approach alone may not be most effective in making the best use of data to solve the problem. He suggests employing algorithmic models to improve prediction. Algorithms refer to a sequence of computational and/or mathematical programs to solve the problem. The goal of algorithmic models is to identify an algorithm that operates on predictor variables (x) to best predict the response variable (y).\n\n\n8.1.2 Machine learning\nThe ultimate goal of data modeling is to explain and predict the variable of interest using data. Machine learning is to achieve this goal using computer algorithms in particular to more effectively make the prediction and solve the problem. According to Carnegie Mellon Computer Science professor Tom M. Mitchell, machine learning is the study of computer algorithms that allow computer programs to automatically improve through experience. To statisticians, the “improve through experience” part is the process of validation or cross validation. Learning can be done through repeated exercises to understand data. This involves having computer or statistics programs do repeated estimations, like human learns from experience and improve actions and decisions. This is called the training process in machine learning.\n“A computer algorithm/program is said to learn from performance measure P and experience E with some class of tasks T if its performance at tasks in T, as measured by P, improves with experience E.” -Tom M. Mitchell.\n\n\n\nTom M. Mitchell\n\n\n \n\n\n Example: Multi-dimensional plot of data\n\n8.1.2.1 Recommended R Resources:\n\nThe R Journal\nIntroduction to R by W. N. Venables, D. M. Smith and the R Core Team\nIntroduction to R Seminar at UCLA\nGetting Started in Data Analysis using Stata and R by Data and Statistical Services, Princeton University ## Illustration: Taiwan election data using caret package"
  }
]