[
  {
    "objectID": "module4.html",
    "href": "module4.html",
    "title": "6  Teaching with AI – Classroom Policies and Ethics",
    "section": "",
    "text": "7 Module 4: Teaching with AI – Classroom Policies and Academic Integrity\nAI tools like ChatGPT present both opportunities and challenges in the classroom. Educators are grappling with questions of if, when, and how students should (or shouldn’t) use AI in coursework. In this module, we examine emerging classroom policies regarding AI assistance, highlighting examples from the University of Michigan, Carnegie Mellon University (CMU), and University of Texas (UT). We also discuss how to communicate these policies to students to uphold academic integrity while embracing innovation.\n1. The Spectrum of AI Policies: Universities and instructors have staked out a range of positions: - Full Prohibition: Some syllabi explicitly ban any use of generative AI in coursework. Example: A strict policy might state that “using ChatGPT or any generative AI for assignments is considered academic misconduct” oai_citation:37‡cmu.edu. Indeed, CMU’s Eberly Center shares an example policy where “any AI-generated content, if passed off as your own, violates the academic integrity policy” oai_citation:38‡cmu.edu. - Allowed with Disclosure: Other policies allow AI with conditions. For instance, an instructor might encourage AI for idea generation or editing, provided students cite the AI’s contributions and provide the prompts used. A CMU/Harvard example says students are “fully encouraged to use generative AI” but must acknowledge usage, include the exact prompt and AI response in an appendix, and note that inaccurate info from AI will lose credit oai_citation:39‡cmu.edu. This approach treats AI like a source that must be cited. - Encouraged Integration: A few go further to integrate AI into learning objectives. E.g., a Michigan guideline suggests instructors could explicitly assign tasks using AI for learning, under principles of transparency and responsibility oai_citation:40‡genai.umich.edu.\nAt University of Michigan, guidance emphasizes four principles for GenAI use: GenAI should enhance learning (not replace it), students remain responsible for all content, transparency in use is required, and misconduct definitions must be clear oai_citation:41‡genai.umich.edu. They outline three broad policy categories: AI use encouraged, allowed with attribution, or banned as misconduct oai_citation:42‡genai.umich.edu. UT Austin’s teaching center similarly notes that no new policy is needed – using uncredited AI is already cheating under existing rules – but urges faculty to decide and declare their stance clearly oai_citation:43‡ctl.utexas.edu.\n2. Rationale and Challenges: Why might an instructor choose a strict vs. lenient policy? - Equity: Not all students may have equal access to paid AI tools – if allowed, ensure access is fair (UT notes the importance of equitable access if AI is used oai_citation:44‡ctl.utexas.edu). - Skill development: Over-reliance on AI for writing or coding might hinder learning. One Michigan recommendation is to “protect the cognitive dimension of learning” – i.e. ensure the student is still doing the critical thinking oai_citation:45‡genai.umich.edu. An assignment meant to practice writing should not be entirely done by AI. - Academic integrity: Clear lines are needed. Students should know if an AI is treated like a calculator (allowed tool) or like a friend giving answers (potential collusion). A middle ground policy must specify what is acceptable help (e.g., “AI can be used to correct grammar or generate ideas, but the core analysis must be your own”). - Preparation for future: Some argue we do students a disservice by banning AI, since they’ll likely use it in jobs. Teaching how to use it responsibly might be better. This sentiment is echoed in many places, e.g., UT’s CTL suggests if adopted, teach students to use AI to “expand rather than impede” their skills oai_citation:46‡ctl.utexas.edu.\n3. Example Policies from Michigan, CMU, UT: - Michigan (U-M): They haven’t imposed a single university-wide rule, but provide template syllabus statements. For example, one U-M sample statement might read: “Specific uses of GenAI (idea generation, editing) are allowed with proper citation of the tool and verification of content. All sources and AI assistance must be openly acknowledged.” oai_citation:47‡genai.umich.edu. They stress no use of AI should go unacknowledged, and students are responsible for any errors AI introduces. - CMU: The Eberly Center shared multiple examples (as we saw). One balanced example (their Example 5) allows AI for some assignments but not others: if an assignment allows AI, it will be stated; otherwise assume it’s disallowed by default oai_citation:48‡cmu.edu oai_citation:49‡cmu.edu. All AI use must be cited, and failing to follow these rules is an integrity violation. - UT Austin: UT’s Center for Teaching and Learning provides sample wordings ranging from “No AI allowed” to “AI allowed with citation” to “AI encouraged as learning tool.” They remind that university policy already covers uncredited assistance as plagiarism oai_citation:50‡ctl.utexas.edu. One UT sample: “This course assumes all submitted work is the student’s own. Uncredited use of AI-generated material is a violation of the honor code” – making clear that if AI is used, it must be credited just like any source.\n4. Hands-On Exercise 4: Drafting an AI Policy. Now, participants will craft a short AI policy for a hypothetical course they teach, using the examples as inspiration: - Decide on the stance (ban, limit, or encourage). Consider the course level and outcomes (e.g., an introductory stats class might discourage AI on problem sets to ensure students learn fundamentals, whereas a project-based class might allow it for efficiency). - Write 3-4 sentences of syllabus text. It should include: whether AI tools are allowed, for what purposes, and the requirement to acknowledge any use of AI. - For instance, “In this course, you may use AI tools like ChatGPT for brainstorming and editing your essays, but you must credit these tools (e.g., “ChatGPT assisted in phrasing this argument”) and remain responsible for verifying the accuracy of any AI-generated content. Unauthorized use of AI to generate complete answers or analysis is prohibited and will be treated as academic misconduct.” - Once everyone drafts their policy, we’ll share a few and discuss: Are they clear? Would students understand what is allowed? Do they uphold academic standards? We’ll also see how different the policies are, reflecting each instructor’s comfort level and discipline norms.\n5. Light Humor Break: Professors have joked, “I wish I had an AI to grade these policies about AI!” In seriousness, tools do exist to detect AI writing (with mixed success) – but an open policy with disclosure might reduce the need for playing detective.\n6. Final Thoughts on Policies: Crafting AI policies is now a key part of syllabus design. The best policies align with learning goals and are communicated clearly to students. As Michigan’s guidance notes, transparency and flexibility are crucial oai_citation:51‡genai.umich.edu oai_citation:52‡genai.umich.edu. By setting the ground rules, we can allow students to benefit from AI’s assistance while maintaining integrity and ensuring they still learn the material. In Module 5, we will look beyond the classroom to how faculty themselves can continue to grow with AI and adapt curricula.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Teaching with AI – Classroom Policies and Ethics</span>"
    ]
  },
  {
    "objectID": "module4.html#references",
    "href": "module4.html#references",
    "title": "6  Teaching with AI – Classroom Policies and Ethics",
    "section": "7.1 References",
    "text": "7.1 References\n\nUniversity of Michigan (Center for Academic Innovation). 2023. “Generative AI Guidance for Instructors.” – Offers principles (transparency, responsibility) and example syllabus statements for AI use oai_citation:53‡genai.umich.edu oai_citation:54‡genai.umich.edu.\nCarnegie Mellon University Eberly Center. 2023. “Examples of Academic Integrity Policies for AI Tools.” – Contains sample policies ranging from total ban to full integration, e.g., requiring prompt/response in appendix if AI used oai_citation:55‡cmu.edu oai_citation:56‡cmu.edu.\nUniversity of Texas at Austin, Center for Teaching and Learning. 2023. “ChatGPT and Generative AI: Sample Syllabus Policy Statements.” – Emphasizes that uncredited AI use falls under existing plagiarism rules oai_citation:57‡ctl.utexas.edu and provides multiple wording options for different stances.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Teaching with AI – Classroom Policies and Ethics</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Data Science with AI",
    "section": "",
    "text": "Workshop Overview: Welcome to “Teaching Data Science with AI,” a one-day workshop for social and political science educators. This workshop introduces how generative artificial intelligence (GenAI) can be leveraged in data science teaching and research workflows. We will explore modern AI tools – from large language models (LLM) to AI-assisted data methods – and discuss best practices for integrating these tools into curriculum design and research.\n\nModules: Five modules cover topics from research applications of AI, data collection with AI (web scraping and APIs), AI-enhanced analysis and visualization, classroom policies on AI, to faculty AI literacy and curriculum design.\nHands-On Exercises: Each module includes coding exercises (primarily in R, with some Python) to practice using AI tools or techniques in data science tasks.\nAudience: This workshop is aimed at faculty, post-doctoral fellows, and advanced users in difference disciplines from social sciences, business to data sciecne. There is no prerequisite but participants will have some advantage with experience in data analysis and AI models (e.g. Copilot and ChatGPT).\n\n\n“In 20 years following the internet space, we cannot recall a faster ramp in a consumer app,” noted analysts when ChatGPT reached 100 million users just two months post-launch oai_citation:0‡reuters.com. The rapid rise of AI tools like ChatGPT underscores their transformative potential in academia. In this workshop, we’ll learn not only what these tools can do, but how to use them effectively and ethically in teaching and research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Teaching Data Science with AI</span>"
    ]
  },
  {
    "objectID": "ParallelProcessing.html",
    "href": "ParallelProcessing.html",
    "title": "Appendix B — Appendix B: Parallel Processing Techniques",
    "section": "",
    "text": "Some tasks in data science (and AI experiments) can benefit from parallel processing – using multiple CPU cores or machines to run computations simultaneously. While not a focus of our workshop, we include a brief overview for those interested in speeding up heavy workloads (such as scraping many pages or running many simulations).\nB1. Why Parallelize? If a job takes 10 hours on one core, in theory splitting it across 10 cores could finish it in ~1 hour. In practice, parallelization has overhead, but it’s invaluable for tasks like: - Web scraping a large number of pages (to stay polite, you might still throttle per site, but you can scrape multiple sites in parallel). - Running cross-validation or hyperparameter tuning for machine learning models. - Simulations or bootstrapping in statistics.\nB2. Parallel Tools in R: - Base R includes the parallel package. For example, mclapply() on Linux/macOS can apply a function in parallel. On Windows, one can use parLapply() with a cluster. - The future package along with furrr (for parallel map with purrr syntax) provides a more user-friendly approach. For instance:\n\n\nCode\nlibrary(future)\n\n\nRegistered S3 method overwritten by 'future':\n  method               from      \n  all.equal.connection parallelly\n\n\nCode\nplan(multisession, workers = 4)  # use 4 cores\nlibrary(furrr)\nresults &lt;- future_map(1:100, ~ { Sys.sleep(1); .x^2 })",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Appendix B: Parallel Processing Techniques</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 References\nArtificial Intelligence (AI) is rapidly becoming ubiquitous in research and education. The advent of accessible large language models (LLMs) like OpenAI’s ChatGPT has opened new possibilities for how we collect data, analyze information, and even teach students. Social and political scientists are beginning to ask how these AI tools can enhance (or disrupt) traditional research workflows and pedagogy. This introduction provides context for the workshop and previews the modules ahead.\nAI in Data Science and Education: Generative AI’s ability to produce human-like text, code, and visuals on demand represents a paradigm shift. Tasks that once took hours (e.g., writing code or summarizing literature) can now be accelerated with AI assistance oai_citation:2‡insights.som.yale.edu. At the same time, using AI in academia raises questions about reliability, ethics, and skills – issues we will revisit throughout the workshop. Banning AI outright is not a sustainable solution; “banning AI use in college classrooms is a pointless and exhausting endeavor” – instead, we should learn to use it responsibly alongside students oai_citation:3‡facultyfocus.com.\nWorkshop Goals: By the end of this workshop, participants will be able to: - Understand the landscape of AI tools (especially LLMs) and their roles in social science research workflows. - Apply AI assistance in data collection (e.g., web scraping, API access) to gather research data more efficiently. - Integrate AI into data analysis and visualization processes (for example, using GPT to generate code or insights), and critically compare traditional statistical methods with machine learning approaches. - Formulate classroom policies on student use of AI that uphold academic integrity and foster learning, informed by emerging policies at institutions like Michigan, CMU, and UT. - Develop strategies for improving their own AI literacy and redesigning curricula to prepare students for an AI-augmented future.\nStructure: The workshop is organized into five modules, each a mix of lecture, demonstration, and hands-on exercise: 1. AI in Research Workflows: Comparing different AI models and tools, and where they fit in a typical research pipeline. 2. AI-Assisted Data Collection: Using AI and automation for web scraping and API data retrieval. 3. AI for Analysis & Visualization: Enhancing data visualization with GPT and examining a case of regression vs. machine learning. 4. Teaching with AI – Policies and Ethics: Crafting classroom policies for AI usage, with examples from several universities. 5. AI Literacy for Faculty: Building educators’ AI skills and rethinking curriculum design.\nEach module will conclude with a practical exercise. We encourage an open mind and active participation—ask questions and share experiences. Let’s explore how AI can amplify our work as researchers and educators, while also understanding its limitations.\nThroughout the book, references are provided in APSA style (author-date citations in text, full references at chapter end). Now, let’s dive into Module 1, where we examine AI models in research workflows.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#references",
    "href": "introduction.html#references",
    "title": "2  Introduction",
    "section": "",
    "text": "Craig, Madeline. 2025. “Earning Our AI Literacy License.” Faculty Focus (February 12). – Emphasizes the need for faculty to become AI literate rather than banning AI oai_citation:4‡facultyfocus.com oai_citation:5‡facultyfocus.com.\nKovács, Balázs, Gaël Le Mens, Michael Hannan, and Guillem Pros. 2024. “Can ChatGPT Accelerate Social Science Research?” Yale Insights (Jan 31). – Case study where ChatGPT replicated a specialized model’s results in a fraction of the time oai_citation:6‡insights.som.yale.edu.\nReuters. 2023. “ChatGPT sets record for fastest-growing user base.” Reuters Technology News (Feb 2). – Report on ChatGPT reaching 100 million users in 2 months, fastest-ever adoption of a consumer app oai_citation:7‡reuters.com.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "3  Module 1: AI in Research Workflows – Comparing Models and Tools",
    "section": "",
    "text": "How do different AI models fit into our research workflows? In this module, we compare various AI approaches and tools, examining their roles in tasks like data analysis, text generation, and coding assistance. We’ll contrast traditional machine learning models (e.g., BERT-based classifiers) with generative AI (e.g., GPT-4) and discuss how each can be used by social scientists.\n1. AI Model Landscape: Modern AI for research largely revolves around foundation models – very large models trained on broad data. For language tasks, two dominant paradigms are Generative models (like GPT series) and Transformers for understanding (like BERT). In essence, “GPT’s strength lies in generating text, whereas BERT excels in tasks that require a deep understanding of language context” oai_citation:10‡geeksforgeeks.org. Generative models (GPT-3.5, GPT-4, etc.) can produce fluent text or code, making them ideal for drafting reports, summarizing articles, or writing code snippets. BERT-like models, on the other hand, are better at classification or extracting information (e.g., sentiment analysis, named-entity recognition) since they encode meaning bidirectionally rather than generate new text oai_citation:11‡geeksforgeeks.org oai_citation:12‡geeksforgeeks.org.\n2. Research Workflow Integration: Consider a typical research workflow in political science: - Literature Review – AI tools like Elicit or Bing Chat can summarize papers or suggest relevant literature. - Data Collection & Cleaning – We’ll see in Module 2 how AI can help fetch or parse data. - Analysis – AI can assist in coding (through code completion tools or generating analysis scripts) and even in choosing methods (e.g., suggesting whether to use a regression or a classification model for a given problem). - Writing & Dissemination – Generative models can draft portions of manuscripts or create engaging summaries for presentations.\nThroughout these stages, choosing the right AI model is key. Simpler models or scripted rules might suffice for well-defined tasks (and be easier to interpret), whereas an LLM can be powerful for open-ended tasks like interpreting qualitative data. A notable example: Kovács et al. (2024) spent three years training a BERT-based model to measure a concept (“typicality”), only to find that ChatGPT could “duplicate their results at a fraction of the cost” in time and resources oai_citation:13‡insights.som.yale.edu. This doesn’t mean BERT is obsolete – but it highlights how general-purpose GPT models can accelerate certain research tasks dramatically with minimal setup.\n3. Hands-On Exercise 1: Comparing AI Tools on a Task. In this exercise, we will compare outputs from two AI systems on a common task: - Task: Summarize a political news article in one paragraph. - Tool A: A pre-trained BERT-based summarizer (for instance, using an extractive summarization library). - Tool B: GPT-3.5 via an API call. - Procedure: We provide both tools the same article text and examine the summaries produced. Below is pseudo-code in R to illustrate how one might invoke these: ```r # Pseudo-code: comparing a BERT summarizer vs. GPT-3.5 article_text &lt;- “Your article text goes here…” # Using a hypothetical BERT-based model via Python (reticulate or other package) library(reticulate) transformers &lt;- import(“transformers”) summarizer &lt;- transformers\\(pipeline(\"summarization\", model=\"bert-base-uncased\")\n   bert_summary &lt;- summarizer(article_text)[[1]]\\)summary_text\n# Using GPT-3.5 via OpenAI API (assuming API key is set as environment variable) library(openai) # OpenAI API R wrapper gpt_summary &lt;- create_chat_completion( model = “gpt-3.5-turbo”, messages = list(list(role=“user”, content=paste(“Summarize this article:”, article_text))) )\\(choices[[1]]\\)message$content\n# Print and compare summaries cat(“BERT-based Summary:”, bert_summary, “ Summary:”, gpt_summary)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module 1: AI in Research Workflows – Comparing Models and Tools</span>"
    ]
  },
  {
    "objectID": "module5.html",
    "href": "module5.html",
    "title": "7  Module 5: Faculty AI Literacy and Curriculum Redesign",
    "section": "",
    "text": "7.1 References\nThe final module turns the focus onto us, the educators and researchers. How do we stay ahead of the curve with AI, and how might we redesign curricula to prepare students for an AI-infused world? We discuss strategies for improving faculty AI literacy and brainstorm ways to incorporate AI into teaching plans in a pedagogically sound manner.\n1. Why Faculty AI Literacy? Just as students are learning to use AI, faculty need to “earn the right to use AI responsibly” by becoming AI literate themselves oai_citation:63‡facultyfocus.com. Our credibility in guiding students on AI use depends on our own understanding of its capabilities and pitfalls. In 2023-2024, there has been a flurry of efforts: - Nearly half of U.S. states have released some guidance on AI in education, and California even moved to include AI literacy in K-12 curriculum oai_citation:64‡facultyfocus.com. This signals a broad recognition that AI skills are becoming fundamental. - Universities (like those we discussed in Module 4) are offering faculty workshops, resource sites, and communities of practice around AI in teaching oai_citation:65‡umdearborn.edu. - Faculty developers suggest treating AI not as an enemy but as a new tool – one that we should learn alongside our students. This ethos of continuous learning is key: technology will keep evolving, and so must we.\nConcrete steps to boost AI literacy among faculty include: - Taking short courses or tutorials on AI basics. (For example, Google’s “Generative AI for Educators” – a free 2-hour course – or the University of Helsinki’s “Elements of AI” course are recommended resources oai_citation:66‡facultyfocus.com.) - Experimenting with AI in one’s own workflow: try using ChatGPT to draft an email, or use an AI image generator to create a figure for class. Hands-on experimentation demystifies the technology. - Reading case studies or research on AI in your discipline – e.g., how are political analysts using AI for polling or how are sociologists using it for text analysis? This can inspire ideas for your curriculum.\n2. Curriculum Redesign for the AI Era: How should we adapt what (and how) we teach? - Integrating AI Topics: Consider adding a module on AI ethics or AI methods in relevant courses. For instance, a research methods class could have a week on “AI-assisted research,” where students try a tool like ChatGPT to generate hypotheses or survey questions and then discuss the quality of those outputs. - Embedding AI Assignments: In data science or statistics courses, include at least one assignment where students use an AI tool. For example, an assignment could be: “Use an AI coding assistant to help you write an analysis script, and then document how it helped and where you had to correct it.” This way, students learn with AI and also learn its limitations. - Assessment and AI: Rethink assessments in light of AI. If take-home essays can be done with AI, perhaps shift to more in-class work or oral exams for certain learning outcomes. Or design assignments that require personal reflection or incorporation of class discussion (harder for AI to do well). Some instructors now ask students to submit “AI + me” portfolios – showing initial AI output and then how the student improved or corrected it, to assess learning. - Example from Carnegie Mellon: A faculty member had students use ChatGPT to generate code for a task, then debug it themselves, turning AI into a learning partner rather than an illicit shortcut oai_citation:67‡cmich.edu.\n3. Hands-On Exercise 5: Curriculum Redesign Brainstorm. Break into small groups (or reflect individually if async) on the following: - Pick a course you teach (or plan to teach) in the social sciences. Identify one aspect of the curriculum that could be updated to incorporate AI. It could be a single assignment, a new module, or a revision of how a topic is taught. - Write down your idea. For example: “In my Political Science Research Methods course, I will add a unit called ‘AI in Content Analysis’. Students will use an NLP AI service to code a set of tweets for sentiment, and we will compare the AI’s coding with human coding to discuss accuracy and bias.” - Consider potential pitfalls: Will students have access to the tools? How will you evaluate their work fairly? How to ensure the use of AI actually enhances learning and isn’t just a gimmick? - Share ideas and get feedback. Each group can present their favorite idea. We will compile these ideas into a shared document – this itself becomes a mini resource for all of us to take home.\n4. Ethical and Future Considerations: As we redesign curriculum, we must also instill ethical AI use in our students. They should graduate not only knowing how to use AI tools, but understanding issues of bias, privacy, and societal impact of AI. This might mean integrating discussions of AI ethics into courses (even technical ones). Moreover, faculty should stay updated: maybe designate an “AI ambassador” in your department who periodically shares new tools or developments with colleagues (since keeping up can feel overwhelming).\n5. A Lighthearted Perspective: Teaching in the AI era can feel like chasing a moving target. One might joke, “By the time I update my syllabus, the AI will have updated itself twice!” Indeed, adaptability is the name of the game. But at its core, teaching has always been about guiding students through complexity – and AI is just the latest complexity. With collective effort and ongoing learning, we can turn AI from a threat to an asset in our teaching toolkit.\n6. Wrap-up: This module caps our workshop by focusing on our growth as educators. Embracing AI in teaching doesn’t happen overnight – consider today a starting point. To continue the journey, tap into faculty communities (for example, the CSU AI Commons or EDUCAUSE initiatives on AI literacy oai_citation:68‡genai.calstate.edu) and keep experimenting. In the conclusion, we will summarize key takeaways from all modules and provide additional resources for your future exploration.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Module 5: Faculty AI Literacy and Curriculum Redesign</span>"
    ]
  },
  {
    "objectID": "module5.html#references",
    "href": "module5.html#references",
    "title": "7  Module 5: Faculty AI Literacy and Curriculum Redesign",
    "section": "",
    "text": "Craig, Madeline. 2025. “Earning Our AI Literacy License.” Faculty Focus – Argues that banning AI is futile and faculty should become AI literate alongside students oai_citation:69‡facultyfocus.com oai_citation:70‡facultyfocus.com.\nU.S. Department of Education. 2023. “Guidance on AI in Education.” – (Referenced in Craig 2025) Highlights the national push for AI literacy in curricula.\nGenerative AI for Educators. 2023. Google – A free online course for teachers to learn how to use AI in teaching oai_citation:71‡facultyfocus.com.\nElements of AI. 2018. University of Helsinki – Free online AI fundamentals course, popular for broad audiences oai_citation:72‡facultyfocus.com.\nEDUCAUSE. 2023. “AI Literacy in Teaching and Learning.” – Describes a faculty development program to integrate AI into course design oai_citation:73‡educause.edu.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Module 5: Faculty AI Literacy and Curriculum Redesign</span>"
    ]
  },
  {
    "objectID": "gc.html",
    "href": "gc.html",
    "title": "Appendix A — Appendix A: Generative Computing & Efficiency Tips",
    "section": "",
    "text": "This technical appendix provides additional notes on generative computing (using generative AI in coding) and some efficiency tips (like memory management) that supplement our workshop content.\nA1. Generative AI in Coding (Additional Examples): In Module 1 and Module 3, we saw how AI can generate code or assist in coding tasks. Here are a few more pointers: - Code Completion Tools: Apart from ChatGPT, tools like GitHub Copilot (powered by OpenAI Codex) can be integrated into IDEs. They work by predicting your next lines of code. Many faculty have found Copilot helpful for writing repetitive code or exploring unfamiliar libraries, effectively acting as a “pair programmer.” - Prompt Engineering for Coding: When using generative AI for coding, how you ask matters. For example, “Write an R function to calculate the Gini coefficient from a numeric vector” will yield a more targeted result than “Give me code for Gini”. Be specific about language (R/Python), input/output, and even style. - Verification: Always test and verify AI-generated code. Treat it like code from a student – likely correct on straightforward tasks, but possibly flawed on complex ones. AI might use outdated functions or assume things. For instance, if ChatGPT suggests using spread() from tidyr (which is deprecated in favor of pivot_wider()), you’ll need to catch that.\nA2. Memory Management and Garbage Collection (for R users): When working with large data or running many AI model calls, you might encounter performance issues. R has a garbage collector (gc() function) that frees up memory. While our one-day workshop exercises are small-scale, if you continue experimenting with AI (say calling an API thousands of times, or fine-tuning models), remember: - Use gc() to explicitly prompt R to clear unused memory if you notice memory bloat. - Monitor memory usage with tools like pryr or base R’s memory.size() (on Windows). - Python users: similarly, be aware of memory when loading big models (use del and gc.collect() from the gc module if needed).\nA3. Dealing with Rate Limits (API efficiency): If you write a script to collect data via an API or to use an AI service repeatedly, you might hit rate limits: - Check if the API allows batch requests. For example, the OpenAI API lets you send multiple prompts in one request (as an array) in some endpoints. - Introduce pauses between requests using Sys.sleep() in R or time.sleep() in Python to avoid triggering caps. - If doing a large scraping job, consider parallel processing (see Appendix B) to speed up while still being polite (distributing load, not exceeding per-IP limits).\nThese generative computing tips and efficiency notes can help when scaling up projects beyond the small examples in the workshop. They weren’t central to our discussions but are here for your reference as you experiment further.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix A: Generative Computing & Efficiency Tips</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "8  Conclusion",
    "section": "",
    "text": "8.1 References\nOver the course of this workshop, we have journeyed through the landscape of “Teaching Data Science with AI.” From understanding AI models and integrating them into research workflows, to practical data collection techniques, through analysis enhancements, and finally grappling with the pedagogical and ethical implications, it’s clear that AI is set to play a significant role in academia.\nKey Takeaways: - AI as Amplifier, Not Replacement: One recurring theme was that AI can amplify our capabilities – helping us work faster or see patterns – but it doesn’t replace the need for human judgment. Whether it was checking a GPT summary against the source, or interpreting a complex model’s output, the human researcher/teacher remains in the loop. - Balance of Innovation and Integrity: We explored how to welcome AI tools in the classroom and research while maintaining rigor and honesty. Clear policies and attribution practices (citing AI like any other source) are part of this balance. - Continuous Learning: The AI field evolves rapidly. Faculty and students alike must adopt a mindset of lifelong learning. What we learned about GPT-4 today might apply to GPT-5 or a completely new tool tomorrow. The specific tools matter less than the adaptable skills – critical thinking, ethical reasoning, and the ability to learn new technologies.\nLooking Forward: In the near future, we might see AI more deeply integrated into research software (imagine your data analysis IDE autocompleting not just code, but suggesting which analysis to run). In education, AI tutors could personalize learning for students, while instructors become orchestrators of human-AI collaboration in the classroom. These prospects are exciting, but they also require us to be vigilant about issues like bias, equity, and the preservation of fundamental skills.\nWe encourage you to take the materials from this workshop – the book chapters, code examples, slide decks, and your own notes – and apply them in your context. Try out that web scraping exercise on a dataset from your research. Modify your syllabus to include an AI-use statement. Share what you learned with colleagues in your department. By doing so, you contribute to a community of practice that is figuring out how to harness AI for the advancement of social science.\nThank you for your active participation and curiosity. As a final light-hearted sendoff: May your code be bug-free, your AI be hallucination-free, and your next grant proposal be co-written by ChatGPT (and get funded)!\nHappy teaching and researching with AI!\n(This concluding section is a synthesis and does not introduce new sources beyond those cited in earlier modules. For further reading, please refer to the references in each module above, which provide a wealth of resources on AI in research and teaching.)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "Teaching Data Science with AI",
    "section": "",
    "text": "(Module2.qmd: Focuses on data collection. It introduces web scraping with rvest oai_citation:27‡rvest.tidyverse.org and Python requests oai_citation:28‡realpython.com, including code examples. Ethics of scraping are mentioned with a light Spider-Man humor. It covers using APIs vs scraping. The exercise involves scraping a Wikipedia table with R and then using an AI (GPT) to summarize the scraped data. This demonstrates AI’s added value in interpretation. References include rvest docs and a Python requests guide.)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>module2.html</span>"
    ]
  }
]