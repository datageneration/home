[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and GenAI: Theory and Applications",
    "section": "",
    "text": "0.1 Welcome\nThe workshop consists of four interconnected modules that progress from theoretical foundations to advanced practical applications, integrating traditional data science methodologies with cutting-edge Generative AI technologies. Each module includes detailed chapter outlines, Quarto-based materials, sample code implementations, and hands-on exercises designed for data science students.\nThis book accompanies the ISSM 2025 workshop titled “Introducation to Data Science” hosted at Academia Sinica, Taiwan.\nIt provides a comprehensive journey through data science theory, practical applications, and the integration of Generative AI.\n## Objectives",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science and GenAI: Theory and Applications</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Data Science and GenAI: Theory and Applications",
    "section": "",
    "text": "Understand foundational theories of data science\nApply data visualization and machine learning techniques\nIntegrate Generative AI into data science workflows\nExplore advanced GenAI systems and future trends",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science and GenAI: Theory and Applications</span>"
    ]
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Data Science and GenAI: Theory and Applications",
    "section": "0.2 Structure",
    "text": "0.2 Structure\n\nWorkshop Introduction\nChapter 1: Theory and Fundamentals\nChapter 2: Visualization, Machine Learning, and GenAI\nChapter 3: GenAI Applications and Collaborative Intelligence\nChapter 4: Advanced GenAI Systems and Future Trends",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science and GenAI: Theory and Applications</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "5  Chapter 4: Advanced Applications – RAG, MCP, and Future Trends",
    "section": "",
    "text": "5.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications – RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter4.html#learning-objectives",
    "href": "chapter4.html#learning-objectives",
    "title": "5  Chapter 4: Advanced Applications – RAG, MCP, and Future Trends",
    "section": "",
    "text": "Implement Retrieval-Augmented Generation (RAG) systems\nUnderstand Model Context Protocol (MCP) applications\nDesign synthetic data generation workflows\nExplore the future of context engineering vs. prompt engineering\nAnticipate future developments in GenAI for data science",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications – RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter4.html#core-content",
    "href": "chapter4.html#core-content",
    "title": "5  Chapter 4: Advanced Applications - RAG, MCP, and Future Trends",
    "section": "5.2 Core Content",
    "text": "5.2 Core Content\n\n5.2.1 4.1 Retrieval-Augmented Generation (RAG)\nRAG enhances large language models (LLMs) by incorporating external knowledge retrieval, addressing limitations of static training data (Lewis et al. 2020; OpenAI 2024; Pinecone 2024).\n\n5.2.1.1 RAG Architecture and Components (Lewis et al. 2020; OpenAI 2024)\nCore Components:\n\nQuery Processing: Converting user queries into retrievable formats\nRetrieval System: Fetching relevant information from external sources\nAugmented Generation: Combining retrieved context with LLM generation\n\nRAG Workflow (Lewis et al. 2020):\nUser Query → Query Processing → Information Retrieval → \nContext Augmentation → LLM Generation → Response\nBenefits of RAG (Lewis et al. 2020; Pinecone 2024):\n\nEnhanced accuracy: Up-to-date, factual information from external sources\nReduced hallucinations: Grounding responses in verified information\nDomain customization: Integration with specialized knowledge bases\nTransparency: Ability to cite sources and provide evidence\n\n\n\n5.2.1.2 Sample RAG Implementation\n\n\nShow the code\n# Simulating RAG System for Data Science Knowledge\n\nlibrary(dplyr)\nlibrary(stringr)\n\n# Create a simple knowledge base\ndata_science_kb &lt;- data.frame(\n  document_id = 1:10,\n  content = c(\n    \"Cross-validation is essential for model evaluation to prevent overfitting\",\n    \"Feature engineering involves creating new variables from existing data\",\n    \"The bias-variance tradeoff is fundamental to machine learning model performance\",\n    \"Exploratory data analysis should always precede model building\",\n    \"Data cleaning typically consumes 80% of a data scientist's time\",\n    \"Correlation does not imply causation in statistical analysis\",\n    \"Ensemble methods often outperform single models in predictive accuracy\",\n    \"Data visualization is crucial for communicating insights effectively\",\n    \"Missing data imputation requires careful consideration of the missingness mechanism\",\n    \"Statistical significance does not always indicate practical significance\"\n  ),\n  keywords = c(\n    \"cross-validation, model evaluation, overfitting\",\n    \"feature engineering, variables, data transformation\",\n    \"bias-variance tradeoff, machine learning, model performance\",\n    \"exploratory data analysis, EDA, model building\",\n    \"data cleaning, preprocessing, time management\",\n    \"correlation, causation, statistical analysis\",\n    \"ensemble methods, model performance, accuracy\",\n    \"data visualization, communication, insights\",\n    \"missing data, imputation, missingness\",\n    \"statistical significance, practical significance, interpretation\"\n  ),\n  relevance_score = runif(10, 0.7, 1.0)\n)\n\n# RAG retrieval function\nrag_retrieval &lt;- function(query, knowledge_base, top_k = 3) {\n  query_words &lt;- tolower(unlist(strsplit(query, \"\\\\s+\")))\n  kb_with_scores &lt;- knowledge_base %&gt;%\n    rowwise() %&gt;%\n    mutate(\n      query_relevance = sum(sapply(query_words, function(word) {\n        grepl(word, tolower(content)) + grepl(word, tolower(keywords))\n      }))\n    ) %&gt;%\n    arrange(desc(query_relevance)) %&gt;%\n    slice_head(n = top_k)\n  return(kb_with_scores)\n}\n\n# RAG generation function\nrag_generate_response &lt;- function(query, retrieved_docs) {\n  context &lt;- paste(retrieved_docs$content, collapse = \" \")\n  response &lt;- paste0(\n    \"Based on the retrieved knowledge: \",\n    \"\\n\\nQuery: \", query,\n    \"\\n\\nRelevant Information:\\n\",\n    paste(retrieved_docs$content, collapse = \"\\n\"),\n    \"\\n\\nSynthesized Response: \",\n    \"The retrieved documents indicate that \", \n    tolower(substr(query, 1, nchar(query)-1)),\n    \" involves multiple considerations from the data science literature.\"\n  )\n  return(list(\n    response = response,\n    sources = retrieved_docs$document_id,\n    context_used = context\n  ))\n}\n\n# Demonstrate RAG system\nuser_query &lt;- \"How should I evaluate my machine learning model?\"\nretrieved &lt;- rag_retrieval(user_query, data_science_kb)\nrag_response &lt;- rag_generate_response(user_query, retrieved)\n\ncat(\"RAG System Response:\\n\")\n\n\nRAG System Response:\n\n\nShow the code\ncat(rag_response$response)\n\n\nBased on the retrieved knowledge: \n\nQuery: How should I evaluate my machine learning model?\n\nRelevant Information:\nThe bias-variance tradeoff is fundamental to machine learning model performance\nExploratory data analysis should always precede model building\nCross-validation is essential for model evaluation to prevent overfitting\nEnsemble methods often outperform single models in predictive accuracy\nFeature engineering involves creating new variables from existing data\nData cleaning typically consumes 80% of a data scientist's time\nCorrelation does not imply causation in statistical analysis\nData visualization is crucial for communicating insights effectively\nMissing data imputation requires careful consideration of the missingness mechanism\nStatistical significance does not always indicate practical significance\n\nSynthesized Response: The retrieved documents indicate that how should i evaluate my machine learning model involves multiple considerations from the data science literature.\n\n\n\n\n\n5.2.2 4.2 Model Context Protocol (MCP)\nMCP standardizes how LLMs interact with external data sources and tools, providing a structured approach to context management (Lewis et al. 2020; OpenAI 2024; LangChain 2024).\n\n5.2.2.1 MCP vs. RAG Comparison (LangChain 2024)\n\n\n\n\n\n\n\n\nAspect\nRAG\nMCP\n\n\n\n\nPrimary Focus\nInformation retrieval and augmentation\nStandardized tool/data interaction\n\n\nArchitecture\nRetrieval → Augmentation → Generation\nProtocol-based communication\n\n\nData Integration\nVector databases, semantic search\nAPIs, databases, file systems\n\n\nStandardization\nImplementation-specific\nProtocol-standardized\n\n\nUse Cases\nKnowledge-based Q&A, research\nTool integration, workflow automation\n\n\n\n\n\n5.2.2.2 MCP Components (OpenAI 2024; LangChain 2024)\n\nMCP Client: Host application interface (AI chatbot)\nMCP Server: Data source or tool provider\nMCP Tools: Specific functions exposed to clients\nProtocol Specification: Standardized communication format\n\n\n\n\n5.2.3 4.3 Synthetic Data Generation\nSynthetic data generation using GenAI addresses data scarcity, privacy concerns, and bias issues in traditional datasets (Graphite 2025; Decodo 2025; Kaur et al. 2022).\n\n5.2.3.1 Types of Synthetic Data Generation (Decodo 2025; Kaur et al. 2022)\n1. Generative AI Models (Graphite 2025; Kaur et al. 2022)\n\nGANs (Generative Adversarial Networks): Adversarial training for realistic data generation\nVAEs (Variational Autoencoders): Probabilistic approach to data generation\nGPT-based Models: Language model approach to tabular data synthesis\n\n2. Rule-based Generation (Decodo 2025)\n\nUser-defined business rules and constraints\nDeterministic generation based on specified parameters\nMaintaining statistical relationships while ensuring privacy\n\nBenefits of Synthetic Data (Graphite 2025; Kaur et al. 2022; Decodo 2025)\n\nPrivacy protection: No real PII or sensitive information exposed\nData augmentation: Increasing dataset size for improved model training\nBias mitigation: Creating balanced datasets for underrepresented groups\nTesting and development: Safe data for software testing and development\n\n\n\n5.2.3.2 Sample Synthetic Data Generation\n\n\nShow the code\n# Synthetic Data Generation for Data Science Training\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\ngenerate_synthetic_customers &lt;- function(n = 1000, seed = 123) {\n  set.seed(seed)\n  correlation_matrix &lt;- matrix(c(\n    1.0, 0.6, -0.4, 0.3,\n    0.6, 1.0, -0.2, 0.5,\n    -0.4, -0.2, 1.0, -0.1,\n    0.3, 0.5, -0.1, 1.0\n  ), nrow = 4)\n  base_vars &lt;- mvrnorm(n, mu = c(35, 50000, 25, 3), Sigma = correlation_matrix * 100)\n  synthetic_data &lt;- data.frame(\n    customer_id = 1:n,\n    age = pmax(18, pmin(80, round(base_vars[,1]))),\n    income = pmax(20000, round(base_vars[,2])),\n    experience_years = pmax(0, pmin(40, round(base_vars[,3]))),\n    satisfaction_score = pmax(1, pmin(5, round(base_vars[,4])))\n  ) %&gt;%\n    mutate(\n      churn_probability = plogis(-2 + 0.02*age - 0.00003*income + \n                                 0.05*experience_years - 0.8*satisfaction_score + \n                                 rnorm(n, 0, 0.5)),\n      churn = rbinom(n, 1, churn_probability),\n      segment = case_when(\n        income &gt; 70000 & satisfaction_score &gt;= 4 ~ \"Premium\",\n        income &gt; 45000 & satisfaction_score &gt;= 3 ~ \"Standard\",\n        TRUE ~ \"Basic\"\n      )\n    ) \n  return(synthetic_data)\n}\n\nsynthetic_customers &lt;- generate_synthetic_customers(1000)\ncat(\"Synthetic Data Summary:\\n\")\n\n\nSynthetic Data Summary:\n\n\nShow the code\nsummary(synthetic_customers)\n\n\n  customer_id          age            income      experience_years\n Min.   :   1.0   Min.   :18.00   Min.   :49975   Min.   : 0.00   \n 1st Qu.: 250.8   1st Qu.:29.00   1st Qu.:49994   1st Qu.:18.00   \n Median : 500.5   Median :35.00   Median :50000   Median :25.00   \n Mean   : 500.5   Mean   :35.42   Mean   :50000   Mean   :24.43   \n 3rd Qu.: 750.2   3rd Qu.:42.00   3rd Qu.:50007   3rd Qu.:32.00   \n Max.   :1000.0   Max.   :67.00   Max.   :50032   Max.   :40.00   \n satisfaction_score churn_probability       churn         segment         \n Min.   :1.000      Min.   :0.0006705   Min.   :0.000   Length:1000       \n 1st Qu.:1.000      1st Qu.:0.0041187   1st Qu.:0.000   Class :character  \n Median :3.000      Median :0.0194273   Median :0.000   Mode  :character  \n Mean   :2.996      Mean   :0.0482846   Mean   :0.049                     \n 3rd Qu.:5.000      3rd Qu.:0.0817124   3rd Qu.:0.000                     \n Max.   :5.000      Max.   :0.2999137   Max.   :1.000                     \n\n\nShow the code\nggplot(synthetic_customers, aes(x = income, y = satisfaction_score, color = factor(churn))) +\n  geom_point(alpha = 0.6) +\n  facet_wrap(~segment) +\n  theme_minimal() +\n  labs(title = \"Synthetic Customer Data Validation\",\n       subtitle = \"Checking realistic relationships between variables\",\n       color = \"Churn\")\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.4 4.4 Context Engineering vs. Prompt Engineering\nThe evolution from prompt engineering to context engineering represents a fundamental shift in AI interaction paradigms (Lakera AI 2025; Contextual AI 2024; Prompting Guide 2025).\n\n5.2.4.1 Context Engineering Definition (Lakera AI 2025; Contextual AI 2024)\nContext engineering is the discipline of designing and building dynamic systems that provide the right information and tools, in the right format, at the right time, to give an LLM everything it needs to accomplish a task (Lakera AI 2025).\n\n\n5.2.4.2 Key Differences (Lakera AI 2025; Contextual AI 2024; Prompting Guide 2025)\n\n\n\n\n\n\n\n\nAspect\nPrompt Engineering\nContext Engineering\n\n\n\n\nScope\nSingle prompt optimization\nSystem-level design\n\n\nFocus\nHow to ask\nWhat information to provide\n\n\nApproach\nString crafting\nInformation architecture\n\n\nContext Window\nLimited consideration\nCentral design consideration\n\n\nPersistence\nStateless interactions\nStateful system design\n\n\nComplexity\nSimple to moderate\nSystem-level complexity\n\n\n\n\n\n5.2.4.3 Context Engineering Components (Lakera AI 2025; Prompting Guide 2025)\n\nInformation Architecture:\n\nInstructions/System prompts\nUser input and queries\nShort-term memory (conversation history)\nLong-term memory (persistent knowledge)\nRetrieved information (RAG systems)\nAvailable tools and their definitions\nStructured output specifications\n\nDynamic Context Management:\n\nReal-time context optimization\nToken limit management\nInformation prioritization\nContext compression techniques\n\n\n\n\n\n5.2.5 4.5 Future Trends in GenAI for Data Science\n\n5.2.5.1 Emerging Paradigms (Anthropic 2024; Bito 2025; Dallas Data Science Academy 2025)\n1. Agentic AI Systems (Anthropic 2024)\n\nAutonomous AI agents for complex data science workflows\nMulti-agent collaboration for large-scale analysis projects\nAdaptive learning and improvement capabilities\n\n2. Multimodal Integration (Monterail 2023)\n\nIntegration of text, images, audio, and structured data analysis\nCross-modal insight generation and validation\nUnified analytical frameworks for diverse data types\n\n3. Democratization Acceleration (Bito 2025; Aisera 2025)\n\nNatural language programming interfaces becoming mainstream\nReduced technical barriers for non-expert users\nAutomated ML pipeline generation and optimization\n\n4. Ethical AI and Responsible Development (Bito 2025)\n\nEnhanced bias detection and mitigation systems\nTransparent AI decision-making processes\nResponsible AI frameworks for data science applications",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications - RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter4.html#hands-on-exercise-4-advanced-genai-applications",
    "href": "chapter4.html#hands-on-exercise-4-advanced-genai-applications",
    "title": "5  Chapter 4: Advanced Applications – RAG, MCP, and Future Trends",
    "section": "5.6 Hands-on Exercise 4: Advanced GenAI Applications",
    "text": "5.6 Hands-on Exercise 4: Advanced GenAI Applications\nCapstone Project: Students implement an integrated GenAI data science system:\n\nRAG Implementation: Build a knowledge retrieval system for domain-specific queries\nSynthetic Data Pipeline: Create realistic synthetic datasets for analysis\nContext Engineering: Design dynamic context management for sustained AI interaction\nCollaborative Workflow: Demonstrate human-AI collaboration throughout the process\nFuture Visioning: Propose innovative applications of emerging GenAI technologies",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications – RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "5  Chapter 4: Advanced Applications – RAG, MCP, and Future Trends",
    "section": "5.7 References",
    "text": "5.7 References\n\nArgyle, Lisa P., Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and David Wingate. “Out of one, many: Using language models to simulate human samples.” Political Analysis 31, no. 3 (2023): 337-351. Aisera. 2025. “What is Human AI Collaboration?” Aisera Blog, May 29. https://aisera.com/blog/human-ai-collaboration/.\n\nAnthropic. 2024. “Agentic AI: The Next Frontier.” Anthropic Blog, April 17. https://www.anthropic.com/blog/agentic-ai.\nBito. 2025. “AI Documentation Generator.” Bito AI. https://bito.ai/blog/ai-documentation-generator/.\nContextual AI. 2024. “What is Context Engineering?” Contextual AI Blog, February 12. https://contextual.ai/blog/context-engineering.\nDallas Data Science Academy. 2025. “Integrating Generative AI in Data Science Projects.” Dallas Data Science Academy Blog, January 6. https://dallasdatascienceacademy.org/blog/integrating-generative-ai-in-data-science-projects.\nDecodo. 2025. “How to Use LLM for Data Analysis: A Comprehensive Guide.” Decodo Blog, April 15. https://decodo.com/blog/llm-for-data-analysis.\nGraphite. 2025. “Understanding ‘Vibe Coding,’ the Future of AI-Driven Development.” Graphite Blog, July 8. https://graphite.dev/guides/understanding-vibe-coding.\nKaur, Harleen, et al. 2022. “A Comprehensive Review on Synthetic Data Generation for Machine Learning.” ACM Computing Surveys 55(9): 1-38.\nLakera AI. 2025. “The Ultimate Guide to Prompt Engineering in 2025.” Lakera AI Blog, May 21. https://www.lakera.ai/blog/prompt-engineering-guide.\nLangChain. 2024. “Model Context Protocol (MCP): A New Standard for AI Tooling.” LangChain Docs. https://docs.langchain.com/docs/mcp.\nLewis, Patrick, et al. 2020. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” Advances in Neural Information Processing Systems 33: 9459–9474.\nMonterail. 2023. “AI-Powered Coding Assistants: Best Practices to Boost Software Development.” Monterail Blog, September 11. https://www.monterail.com/blog/ai-powered-coding-assistants-best-practices.\nOpenAI. 2024. “Retrieval-Augmented Generation (RAG) and Model Context Protocol (MCP): Technical Overview.” OpenAI Technical Reports. https://openai.com/research/rag-mcp.\nPinecone. 2024. “What is Retrieval-Augmented Generation (RAG)?” Pinecone Blog, March 12. https://www.pinecone.io/learn/retrieval-augmented-generation/.\nPrompting Guide. 2025. “General Tips for Designing Prompts.” Prompt Engineering Guide, June 7. https://www.promptingguide.ai/introduction/tips.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications – RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "",
    "text": "4 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#learning-objectives",
    "href": "chapter2.html#learning-objectives",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "",
    "text": "Master data visualization theory and principles (Tufte 2001; Ware 2021)\nUnderstand machine learning fundamentals in the data science context (James et al. 2021; Murphy 2022)\nExplore how Generative AI enhances traditional data science workflows (Davenport et al. 2024; Slack 2024)\nDemonstrate GenAI applications in data collection and knowledge mining (Prompting Guide 2025; Dallas Data Science Academy 2025)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#core-content",
    "href": "chapter2.html#core-content",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "3.2 Core Content",
    "text": "3.2 Core Content\n\n3.2.1 2.1 Data Visualization Theory and Practices\nEffective data visualization follows established principles rooted in cognitive psychology and design theory (Tufte 2001; Ware 2021; Few 2012). The ultimate goal is storytelling—conveying information about data as efficiently as possible (Tufte 2001).\n\n3.2.1.1 Fundamental Principles\nGestalt Theory Applications (Ware 2021; Tufte 2001):\n\nSimilarity: Grouping objects with similar visual properties\nProximity: Objects close together are perceived as related\nEnclosure: Elements within boundaries are seen as grouped\nConnectedness: Connected elements are perceived as related\n\nTufte’s Information-to-Ink Ratio (Tufte 2001; Few 2012):\nMaximize the data-to-ink ratio by eliminating non-essential visual elements that do not represent data (Tufte 2001).\n\n\n3.2.1.2 Sample Code Exercise\n\n\nShow the code\n# Demonstrating Visualization Principles\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# Poor visualization example (high ink-to-data ratio)\npoor_plot &lt;- ggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point(size = 3, color = \"red\") +\n  theme_gray() +\n  theme(\n    panel.grid.major = element_line(size = 1),\n    panel.grid.minor = element_line(size = 0.5),\n    panel.background = element_rect(fill = \"lightgray\"),\n    plot.background = element_rect(fill = \"yellow\"),\n    axis.text = element_text(size = 12, face = \"bold\"),\n    axis.title = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5)\n  ) +\n  labs(title = \"HORSEPOWER VS MILES PER GALLON!!!\",\n       x = \"MILES PER GALLON\",\n       y = \"HORSEPOWER\")\n\n# Improved visualization (optimized ink-to-data ratio)\ngood_plot &lt;- ggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point(alpha = 0.7) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"gray90\", size = 0.3)\n  ) +\n  labs(title = \"Engine efficiency vs power\",\n       x = \"Miles per gallon\",\n       y = \"Horsepower\",\n       caption = \"Source: 1974 Motor Trend\")\n\n# Combine plots for comparison\npoor_plot + good_plot + \n  plot_annotation(title = \"Visualization Design Principles Comparison\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 2.2 Machine Learning Fundamentals for Data Science\nMachine learning provides the methodological foundation for predictive analytics and pattern recognition in data science (James et al. 2021; Murphy 2022). Understanding core ML concepts is essential for effective data science practice.\n\n3.2.2.1 Key ML Concepts (James et al. 2021; Murphy 2022)\nSupervised Learning:\n\nRegression: Predicting continuous outcomes\nClassification: Predicting categorical outcomes\nModel validation and cross-validation techniques\n\nUnsupervised Learning:\n\nClustering: Discovering hidden patterns in data\nDimensionality reduction: Simplifying complex datasets\nAssociation rules: Finding relationships between variables\n\nModel Evaluation:\n\nBias-variance tradeoff considerations (Geman et al. 1992; Domingos 2012)\nTraining, validation, and test set methodologies\nPerformance metrics selection and interpretation\n\n\n\n3.2.2.2 Sample Code Exercise\n\n\nShow the code\n# Machine Learning Pipeline Demonstration\nlibrary(caret)\nlibrary(randomForest)\n\n# Data preparation\ndata(Boston, package = \"MASS\")\nset.seed(123)\n\n# Split data\ntrain_index &lt;- createDataPartition(Boston$medv, p = 0.7, list = FALSE)\ntrain_data &lt;- Boston[train_index, ]\ntest_data &lt;- Boston[-train_index, ]\n\n# Model training with cross-validation\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\n# Linear regression model (Data Modeling Culture approach)\nlm_model &lt;- train(medv ~ ., \n                  data = train_data,\n                  method = \"lm\",\n                  trControl = ctrl)\n\n# Random Forest model (Algorithmic Modeling Culture approach)\nrf_model &lt;- train(medv ~ ., \n                  data = train_data,\n                  method = \"rf\",\n                  trControl = ctrl)\n\n# Model comparison\nresults &lt;- resamples(list(Linear = lm_model, RandomForest = rf_model))\nsummary(results)\n\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: Linear, RandomForest \nNumber of resamples: 5 \n\nMAE \n                 Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLinear       3.061024 3.108807 3.122744 3.362207 3.734839 3.783622    0\nRandomForest 1.639452 2.338742 2.441751 2.353005 2.585118 2.759961    0\n\nRMSE \n                 Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLinear       4.028769 4.314112 4.578507 4.685958 5.115352 5.393048    0\nRandomForest 2.078202 3.666595 3.828959 3.577498 3.974632 4.339103    0\n\nRsquared \n                  Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLinear       0.6701489 0.6926935 0.7686543 0.7370820 0.7749271 0.7789863    0\nRandomForest 0.7775192 0.7850751 0.8375004 0.8491234 0.9223781 0.9231443    0\n\n\nShow the code\n# Visualization of model performance\nggplot(results) + \n  theme_minimal() +\n  labs(title = \"Model Performance Comparison: Two Cultures Approach\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 2.3 Data Science and Generative AI Integration\nThe integration of Generative AI into data science workflows represents a paradigm shift, enabling new approaches to data analysis, interpretation, and insight generation (Davenport et al. 2024; Slack 2024; Monterail 2023).\n\n3.2.3.1 GenAI Applications in Data Science (Davenport et al. 2024; IBM 2025)\nContent Generation and Analysis:\n\nAutomated report generation and summarization (Davenport et al. 2024)\nCode generation and debugging assistance (IBM 2025)\nData documentation and metadata creation (Prompting Guide 2025)\n\nEnhanced Analytics:\n\nNatural language querying of datasets (Slack 2024)\nAutomated feature engineering suggestions (IBM 2025)\nIntelligent data cleaning and preprocessing (Davenport et al. 2024)\n\nKnowledge Discovery:\n\nPattern identification in unstructured data (Prompting Guide 2025)\nAutomated hypothesis generation (IBM 2025)\nInsight extraction from complex datasets (Dallas Data Science Academy 2025)\n\n\n\n\n3.2.4 2.4 GenAI Enhanced Data Collection and Knowledge Mining\nGenerative AI transforms traditional data collection and knowledge extraction processes (Prompting Guide 2025; Dallas Data Science Academy 2025; Prompting Guide 2025).\n\n3.2.4.1 Data Collection Enhancement (Prompting Guide 2025; Goodale 2025)\n\nAutomated survey generation and optimization\nIntelligent sampling strategy recommendations\nReal-time data quality assessment and improvement\nSynthetic data generation for augmenting datasets (Graphite 2025; Decodo 2025)\n\n\n\n3.2.4.2 Knowledge Mining Applications (Prompting Guide 2025; Dallas Data Science Academy 2025)\n\nSemantic analysis of unstructured documents\nAutomated extraction of insights from research literature\nDynamic knowledge base construction and maintenance\nIntelligent information retrieval and recommendation systems\n\n\n\n3.2.4.3 Sample Code Exercise\n\n\nShow the code\n# Simulating GenAI-Enhanced Data Analysis Workflow\nlibrary(httr)\nlibrary(jsonlite)\n\n# Function to simulate AI-assisted data analysis\nai_assisted_analysis &lt;- function(data, question) {\n  # This would interface with actual GenAI APIs in practice\n  \n  # Simulate data summary generation\n  summary_stats &lt;- data %&gt;%\n    summarise_all(list(\n      mean = ~mean(., na.rm = TRUE),\n      median = ~median(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE)\n    ))\n  \n  # Simulate AI-generated insights\n  insights &lt;- list(\n    question = question,\n    data_shape = dim(data),\n    key_variables = names(data),\n    suggested_analysis = \"Based on the data structure, consider correlation analysis and regression modeling\",\n    automated_summary = paste(\"Dataset contains\", nrow(data), \"observations and\", ncol(data), \"variables\")\n  )\n  \n  return(insights)\n}\n\n# Demonstrate AI-assisted workflow\nquestion &lt;- \"What factors influence housing prices in Boston?\"\nai_insights &lt;- ai_assisted_analysis(Boston, question)\nprint(ai_insights)\n\n\n$question\n[1] \"What factors influence housing prices in Boston?\"\n\n$data_shape\n[1] 506  14\n\n$key_variables\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n$suggested_analysis\n[1] \"Based on the data structure, consider correlation analysis and regression modeling\"\n\n$automated_summary\n[1] \"Dataset contains 506 observations and 14 variables\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#hands-on-exercise-2-integrated-data-science-workflow",
    "href": "chapter2.html#hands-on-exercise-2-integrated-data-science-workflow",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "6.1 Hands-on Exercise 2: Integrated Data Science Workflow",
    "text": "6.1 Hands-on Exercise 2: Integrated Data Science Workflow\nChallenge: Students work with a real-world dataset to:\n\nCreate effective visualizations following design principles\nApply both data modeling and algorithmic modeling approaches\nUse simulated GenAI tools for insight generation\nCompare traditional vs. AI-enhanced analytical workflows\nDocument findings using Quarto for reproducible research",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#references",
    "href": "chapter2.html#references",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "6.2 References",
    "text": "6.2 References\nDallas Data Science Academy. 2025. “Integrating Generative AI in Data Science Projects.” Dallas Data Science Academy Blog, January 6. https://dallasdatascienceacademy.org/blog/integrating-generative-ai-in-data-science-projects.\nDavenport, Thomas H., Nitin Mittal, and Ilya Goldin. 2024. “The Impact of Generative AI on Data Science.” DATAVERSITY, May 1. https://www.dataversity.net/the-impact-of-generative-ai-on-data-science/.\nDecodo. 2025. “How to Use LLM for Data Analysis: A Comprehensive Guide.” Decodo Blog, April 15. https://decodo.com/blog/llm-for-data-analysis.\nDomingos, Pedro. 2012. “A Few Useful Things to Know about Machine Learning.” Communications of the ACM 55(10): 78-87.\nFew, Stephen. 2012. Show Me the Numbers: Designing Tables and Graphs to Enlighten. 2nd ed. Burlingame, CA: Analytics Press.\nGeman, Stuart, Elie Bienenstock, and René Doursat. 1992. “Neural Networks and the Bias/Variance Dilemma.” Neural Computation 4(1): 1-58.\nGoodale, Tom. 2025. “Using Generative AI to Help with Statistical Test Selection and Analysis.” MSOR Connections 22(3): 10-17. https://journals.gre.ac.uk/index.php/msor/article/download/1485/pdf/7286.\nGraphite. 2025. “Understanding ‘Vibe Coding,’ the Future of AI-Driven Development.” Graphite Blog, July 8. https://graphite.dev/guides/understanding-vibe-coding.\nIBM. 2025. “What is Vibe Coding?” IBM Think Topics, April 8. https://www.ibm.com/think/topics/vibe-coding.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2nd ed. New York: Springer.\nMonterail. 2023. “AI-Powered Coding Assistants: Best Practices to Boost Software Development.” Monterail Blog, September 11. https://www.monterail.com/blog/ai-powered-coding-assistants-best-practices.\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Cambridge, MA: MIT Press.\nPrompting Guide. 2025. “General Tips for Designing Prompts.” Prompt Engineering Guide, June 7. https://www.promptingguide.ai/introduction/tips.\nSlack. 2024. “Collaborative Intelligence: People and AI Working Smarter Together.” Slack Blog, January 15. https://slack.com/blog/collaboration/collaborative-intelligence-people-and-ai-working-smarter-together.\nTufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, CT: Graphics Press.\nWare, Colin. 2021. Information Visualization: Perception for Design. 4th ed. Cambridge, MA: Morgan Kaufmann.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "",
    "text": "4.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "",
    "text": "2.1 Learning Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter1.html#data-generation-processes-and-theory",
    "href": "chapter1.html#data-generation-processes-and-theory",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "2.2 1.1 Data Generation Processes and Theory",
    "text": "2.2 1.1 Data Generation Processes and Theory\n\n2.2.1 What is a Data Generation Process (DGP)?\nA Data Generation Process (DGP) refers to the underlying mechanism or set of rules by which data are produced. According to Jepusto (2025), a DGP is essentially a “recipe” for producing data, specifying how each value in a dataset is determined—whether by random chance, deterministic rules, or a mix of both. Understanding the DGP is crucial for:\n\nModeling: Selecting appropriate statistical models depends on knowing the underlying distribution and process that generated the data.\nSimulation: Simulating realistic data for testing methods or teaching requires specifying a DGP.\nInference: Causal and statistical inference rely on assumptions about how data are produced.\n\n\n2.2.1.1 Why is DGP Important in Data Science Analytics?\n\nDistributional Assumptions: Many models (e.g., regression, classification) assume data follow certain distributions. Knowing the DGP ensures these assumptions are valid.\nSimulation Studies: When evaluating methods, researchers simulate data from a known DGP to assess performance under controlled conditions.\nBias and Variance: The DGP determines the inherent variability and potential biases in data, affecting model evaluation and interpretation.\n\n\n\n2.2.1.2 Example: Simulating DGPs\nBelow are two DGPs—one linear (stochastic) and one deterministic. The charts illustrate how data generated under each process look different.\n\n\nShow the code\nlibrary(tidyverse)\n\n# Linear (stochastic) DGP\nset.seed(42)\nlinear_data &lt;- tibble(\n  x = runif(100, 0, 10),\n  y = 1 + 2 * x + rnorm(100, 0, 1)\n)\n\n# Deterministic DGP\ndeterministic_data &lt;- tibble(\n  x = seq(0, 10, length.out = 100),\n  y = 1 + 2 * x\n)\n\n# Chart 1: Linear (Stochastic) DGP\nggplot(linear_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  labs(title = \"Chart 1: Stochastic Linear DGP\",\n       x = \"x\", y = \"y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nShow the code\n# Chart 2: Deterministic DGP\nggplot(deterministic_data, aes(x = x, y = y)) +\n  geom_line(color = \"firebrick\", size = 1.2) +\n  labs(title = \"Chart 2: Deterministic Linear DGP\",\n       x = \"x\", y = \"y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nExplanation:\n\nChart 1 shows data with random noise, typical of a stochastic process.\nChart 2 shows a perfect line, as every y is exactly determined by x.\n\n\n\n2.2.1.3 Stochastic vs. Deterministic Data Generation\n\nStochastic DGP: Includes randomness; outcomes vary even with the same inputs. Example: Rolling a die, measurement error in experiments.\nDeterministic DGP: No randomness; outcomes are fully determined by inputs. Example: Calculating area of a circle given radius.\n\nKey Differences:\n\nStochastic processes model uncertainty and variability in real-world data.\nDeterministic processes are rare in observational data but common in simulations or controlled settings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-two-cultures-of-statistical-modeling",
    "href": "chapter1.html#the-two-cultures-of-statistical-modeling",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "2.3 1.2 The Two Cultures of Statistical Modeling",
    "text": "2.3 1.2 The Two Cultures of Statistical Modeling\nBreiman (2001) described two main approaches:\n\n2.3.1 Data Modeling Culture (DMC)\n\nAssumes data are generated by a specific stochastic model (e.g., linear regression, logistic regression).\nFocuses on understanding the process, estimating parameters, and testing hypotheses.\nExample: Modeling the effect of education on income using a regression model with specified error terms.\n\n\n\n2.3.2 Algorithmic Modeling Culture (AMC)\n\nTreats the data mechanism as unknown; focuses on predictive accuracy.\nUses flexible algorithms (e.g., random forests, neural networks) that may not provide interpretable parameters.\nExample: Using a random forest to predict customer churn without specifying a generative model.\n\n\n2.3.2.1 Four Key Characteristics\n\nDMC:\n\nInterpretability: Models are designed to be interpretable and explainable.\nInference: Focus on hypothesis testing and parameter estimation.\nAssumptions: Requires strong distributional assumptions.\nCausality: Often used for causal inference.\n\nAMC:\n\nPredictive Accuracy: Main goal is to minimize prediction error.\nFlexibility: Can model complex, nonlinear relationships.\nFewer Assumptions: Makes minimal assumptions about the data.\nBlack-box Nature: Often less interpretable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter1.html#big-data-and-the-five-vs",
    "href": "chapter1.html#big-data-and-the-five-vs",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "2.4 1.3 Big Data and the Five V’s",
    "text": "2.4 1.3 Big Data and the Five V’s\nTraditional big data is characterized by three V’s:\n\nVolume: The amount of data.\nVelocity: The speed at which data are generated and processed.\nVariety: The diversity of data types and sources.\n\nBurt Monroe (2012) adds two more:\n\nVinculation: The degree to which data points are linked or related (e.g., social network data).\nValidity: The accuracy and trustworthiness of the data for the intended purpose.\n\nSummary Table: The Five V’s of Big Data\n\n\n\n\n\n\n\n\nV\nDescription\nExample\n\n\n\n\nVolume\nAmount of data\nTerabytes of tweets\n\n\nVelocity\nSpeed of data creation/processing\nReal-time sensor data\n\n\nVariety\nDifferent types/sources\nText, images, logs\n\n\nVinculation\nConnectedness/linkages among data points\nSocial network graphs\n\n\nValidity\nData quality and relevance\nVerified medical records",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter1.html#found-data-vs.-made-data",
    "href": "chapter1.html#found-data-vs.-made-data",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "2.5 1.4 Found Data vs. Made Data",
    "text": "2.5 1.4 Found Data vs. Made Data\n\n2.5.1 Found Data\n\nDefinition: Data collected for purposes other than the current research question; often passively generated.\nExamples: Social media posts, transaction logs, government administrative data, web scraping, sensor data.\nStrengths: Large volume, real-world behavior, cost-effective.\nLimitations: May lack key variables, less control over data quality, potential for bias.\n\n\n\n2.5.2 Made Data\n\nDefinition: Data collected specifically for research, often via experiments or surveys with a designed protocol.\nExamples: Randomized controlled trials, survey data (e.g., TEDS), laboratory experiments.\nStrengths: High control, tailored to research questions, better for causal inference.\nLimitations: Costly, time-consuming, may be limited in scale.\n\n\n\n2.5.3 Emphasizing Design in Small Data\n\nSmall data projects often require careful design: sampling, measurement, and data collection protocols are critical.\nGood design ensures validity, reliability, and interpretability—especially important when data volume is low.\n\n\n\n2.5.4 Comparison Table: Found Data vs. Made Data\n\n\n\n\n\n\n\n\nAspect\nFound Data\nMade Data\n\n\n\n\nOrigin\nPassive/byproduct of other processes\nPurposeful collection for research\n\n\nControl\nLow\nHigh\n\n\nVolume\nOften large\nTypically small/moderate\n\n\nVariables\nMay lack key variables\nTailored to research needs\n\n\nQuality\nVariable; may lack documentation\nControlled; well-documented\n\n\nBias\nPotentially high\nCan be minimized by design\n\n\nExample\nSocial media, government records\nSurveys, experiments",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter1.html#examples",
    "href": "chapter1.html#examples",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "2.6 Examples",
    "text": "2.6 Examples\n\nFound Data:\n\nWeb data\n\nWikipedia tables\nUS government open portal API\nMedia Cloud (news stories)\n\nMade Data:\n\nAmerican National Election Studies (ANES) survey\nRandomized controlled trial survey",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter1.html#summary",
    "href": "chapter1.html#summary",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "2.7 Summary",
    "text": "2.7 Summary\n\nUnderstanding the DGP is foundational for all data science analytics, affecting model choice, simulation, and inference.\nThe distinction between stochastic and deterministic processes underlies much of statistical modeling.\nThe two cultures (DMC and AMC) highlight the tradeoff between interpretability and predictive power.\nBig data is more than just size; vinculation and validity are essential for meaningful analysis.\nFound data and made data each have strengths and limitations—design is especially critical in small, made data projects.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter1.html#hands-on-exercise-1-data-generation-process-analysis",
    "href": "chapter1.html#hands-on-exercise-1-data-generation-process-analysis",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "2.8 Hands-on Exercise 1: Data Generation Process Analysis",
    "text": "2.8 Hands-on Exercise 1: Data Generation Process Analysis\nChallenge: Students identify three datasets without knowing their generation process and must:\n\nIdentify potential DGPs through exploratory analysis\nClassify which “culture” approach would be most appropriate\nDetermine if the data represents big data, small data, found data, or made data\nJustify their analytical strategy choice",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter1.html#suggestions",
    "href": "chapter1.html#suggestions",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "2.9 Suggestions:",
    "text": "2.9 Suggestions:\n\nUS Census data\nTaiwan Election and Democratization Study (TEDS)\nUS government information (https://www.govinfo.gov/)\nTaiwan Open Parliament API (https://www.ly.gov.tw/Pages/List.aspx?nodeid=153)\nMedia Cloud (https://www.mediacloud.org/)\n\n\n2.9.1 References\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16(3): 199-231.\nChen, C.L. Philip, and Chun-Yang Zhang. 2014. “Data-Intensive Applications, Challenges, Techniques and Technologies: A Survey on Big Data.” Information Sciences 275: 314-347.\nDaoud, Adel, and Devdatt Dubhashi. 2020. “Statistical Modeling: The Three Cultures.” Harvard Data Science Review 2(1). https://doi.org/10.1162/99608f92.6b3ba01e.\nGelman, Andrew. 2020. “Reflections on Breiman’s Two Cultures of Statistical Modeling.” Observational Studies 6(1): 15-24.\nHand, David J. 2019. “Aspects of Data Ethics in a Changing World: Where Are We Now?” Big Data 6(3): 176-190.\nJepusto, James. 2025. “Chapter 6: Data-Generating Processes.” In Designing Monte Carlo Simulations in R. https://jepusto.github.io/Designing-Simulations-in-R/data-generating-processes.html.\nKass, Robert E. 2021. “The Two Cultures: Statistics and Machine Learning in Science.” Harvard Data Science Review 3(1). https://doi.org/10.1162/99608f92.ba20f892.\nLazer, David, Ryan Kennedy, Gary King, and Alessandro Vespignani. 2014. “The Parable of Google Flu: Traps in Big Data Analysis.” Science 343(6176): 1203-1205.\nLindstrom, Peter. 2020. “Small Data, Big Impact: How to Get More from Less Data.” MIT Technology Review, March 15.\nMoya, Cristobal. 2009. “Chapter 3: Data Generation Processes.” In Inferential Statistics and Causal Inference. https://bookdown.org/cristobalmoya/iscs_materials/data-generation.html.\nPearl, Judea. 2009. Causality: Models, Reasoning, and Inference. 2nd ed. Cambridge: Cambridge University Press.\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the Digital Age. Princeton: Princeton University Press.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapter2.html#data-visualization-theory-and-practices",
    "href": "chapter2.html#data-visualization-theory-and-practices",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.1 2.1 Data Visualization Theory and Practices",
    "text": "4.1 2.1 Data Visualization Theory and Practices\n\n4.1.1 Theoretical Foundations\n1. Grammar of Graphics (Leland Wilkinson) and ggplot2 (Hadley Wickham)\nThe Grammar of Graphics, developed by Leland Wilkinson, is a framework that describes the building blocks of all statistical graphics. It underpins the design of ggplot2, R’s most popular visualization package, created by Hadley Wickham. This grammar allows users to layer data, aesthetics, geoms, stats, and facets to build complex visualizations from simple components.\n2. Cognitive and Perceptual Theory\n- Gestalt Principles:\n- Similarity: Similar elements are perceived as related. - Proximity: Objects that are close together are grouped. - Enclosure: Boundaries suggest grouping. - Connectedness: Lines or paths imply relationships.\n3. Cleveland & McGill’s Empirical Studies:\nWilliam Cleveland and Robert McGill empirically ranked graphical perception tasks, showing that position along a common scale is interpreted most accurately, followed by length, angle, area, and color. This hierarchy informs best practices: prefer scatterplots or bar charts over pie charts for quantitative comparisons.\n4. Information-to-Ink Ratio (Edward Tufte)\nTufte’s principle encourages maximizing the proportion of “ink” that conveys data, minimizing non-essential elements for clarity and efficiency.\n\n\n4.1.2 From Theory to Application\n\nHans Rosling’s Seminal Work:\nHans Rosling’s Gapminder visualizations exemplify the power of dynamic, multi-dimensional graphics. His animated bubble charts (e.g., life expectancy vs. income) use color, size, and motion to tell compelling data stories and reveal trends over time.\nExample: Building a Multi-Layered Plot with ggplot2\n\n\n\nShow the code\n#|message: false\nlibrary(ggplot2)\nlibrary(gapminder)\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) +\n  geom_point(alpha = 0.6) +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs GDP per Capita\",\n       subtitle = \"Gapminder: Hans Rosling's Classic Visualization\",\n       x = \"GDP per Capita (log scale)\",\n       y = \"Life Expectancy\",\n       size = \"Population\",\n       color = \"Continent\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCan you improve this? Animate it? Or add facets?\nKey Takeaways:\n\nUse position and length for quantitative comparisons (Cleveland & McGill).\nLayer elements to tell richer stories (Grammar of Graphics).\nRemove chartjunk (Tufte).\nUse animation or interactivity for temporal or multidimensional data (Rosling).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#fundamentals-of-data-visualization-theory-and-applications",
    "href": "chapter2.html#fundamentals-of-data-visualization-theory-and-applications",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.2 2.2 Fundamentals of Data Visualization: Theory and Applications",
    "text": "4.2 2.2 Fundamentals of Data Visualization: Theory and Applications\n\n4.2.1 How Visualization Fundamentals Are Developed\n\nTheory:\n\nGrounded in perception, cognition, and information design.\nThe Grammar of Graphics provides a systematic approach to constructing and reasoning about graphics.\nCleveland & McGill’s research guides the choice of chart types and encodings.\n\nApplications:\n\nggplot2 operationalizes the Grammar of Graphics in R.\nReal-world examples (Rosling’s Gapminder, Tufte’s minimalist charts) illustrate the impact of design choices on data understanding.\n\n\n\n\n4.2.2 Example: Comparing Chart Types\n\n\n\n\n\n\n\n\n\nChart Type\nBest For\nPerceptual Strength\nExample Use Case\n\n\n\n\nScatterplot\nCorrelation\nPosition\nLife expectancy vs. GDP\n\n\nBar Chart\nMagnitude\nLength\nCounts by category\n\n\nLine Chart\nTrends over time\nPosition/Angle\nStock prices, time series\n\n\nPie Chart\nPart-whole\nAngle/Area (weak)\nMarket share (use sparingly)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#machine-learning-fundamentals-for-data-science",
    "href": "chapter2.html#machine-learning-fundamentals-for-data-science",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.3 2.3 Machine Learning Fundamentals for Data Science",
    "text": "4.3 2.3 Machine Learning Fundamentals for Data Science\n\n4.3.1 Core Concepts\n\nSupervised Learning: Regression and classification; model validation, cross-validation.\nUnsupervised Learning: Clustering, dimensionality reduction, association rules.\nModel Evaluation: Bias-variance tradeoff, training/validation/test splits, performance metrics.\n\nExample: Two Cultures in Practice\n\nData Modeling Culture (DMC): Linear regression for predicting house prices, focusing on model coefficients and assumptions.\nAlgorithmic Modeling Culture (AMC): Random forest or neural network for the same task, prioritizing predictive accuracy.\n\nSample Code: Model Comparison\n\n\nShow the code\nlibrary(caret)\nlibrary(randomForest)\ndata(Boston, package = \"MASS\")\nset.seed(123)\ntrain_index &lt;- createDataPartition(Boston$medv, p = 0.7, list = FALSE)\ntrain_data &lt;- Boston[train_index, ]\ntest_data &lt;- Boston[-train_index, ]\nctrl &lt;- trainControl(method = \"cv\", number = 5)\nlm_model &lt;- train(medv ~ ., data = train_data, method = \"lm\", trControl = ctrl)\nrf_model &lt;- train(medv ~ ., data = train_data, method = \"rf\", trControl = ctrl)\nresults &lt;- resamples(list(Linear = lm_model, RandomForest = rf_model))\nsummary(results)\n\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: Linear, RandomForest \nNumber of resamples: 5 \n\nMAE \n                 Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLinear       3.061024 3.108807 3.122744 3.362207 3.734839 3.783622    0\nRandomForest 1.639452 2.338742 2.441751 2.353005 2.585118 2.759961    0\n\nRMSE \n                 Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLinear       4.028769 4.314112 4.578507 4.685958 5.115352 5.393048    0\nRandomForest 2.078202 3.666595 3.828959 3.577498 3.974632 4.339103    0\n\nRsquared \n                  Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nLinear       0.6701489 0.6926935 0.7686543 0.7370820 0.7749271 0.7789863    0\nRandomForest 0.7775192 0.7850751 0.8375004 0.8491234 0.9223781 0.9231443    0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#genai-with-machine-learning-and-applications",
    "href": "chapter2.html#genai-with-machine-learning-and-applications",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.5 2.4 GenAI with Machine Learning and Applications",
    "text": "4.5 2.4 GenAI with Machine Learning and Applications\n\n4.5.1 Basics of Prompt Engineering\nPrompt engineering is the practice of crafting effective inputs (prompts) to guide GenAI models (like LLMs) to produce useful outputs. In data science, this means:\n\nAsking for code generation (“Write R code for a scatterplot of mpg vs hp”)\nRequesting data summaries, explanations, or visualizations\nIteratively refining prompts for clarity and specificity\n\nBest Practices: - Be explicit about the task and format you want - Provide context or sample data if possible - Use step-by-step instructions for complex tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#prompt-engineering-workshop",
    "href": "chapter2.html#prompt-engineering-workshop",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.6 Prompt Engineering Workshop",
    "text": "4.6 Prompt Engineering Workshop\n\n\nWhat is Prompt Engineering?\nThe art of crafting inputs to optimize AI-generated responses.\nHelps improve accuracy, relevance, and usability of AI-generated content.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#why-prompt-engineering-matters",
    "href": "chapter2.html#why-prompt-engineering-matters",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.7 Why Prompt Engineering Matters",
    "text": "4.7 Why Prompt Engineering Matters\n\n\nBetter prompts = Better AI responses\nAI models do not “think”; they predict text based on input.\nWell-structured prompts improve clarity, relevance, and accuracy.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#steps-to-improve-prompts",
    "href": "chapter2.html#steps-to-improve-prompts",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.8 Steps to Improve Prompts",
    "text": "4.8 Steps to Improve Prompts\n\n\nBe Specific → Avoid vague questions.\nAdd Context → Define roles or perspectives.\nUse Step-by-Step Instructions → Encourage reasoning.\nTest & Iterate → Adjust prompts based on AI output.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#example-improving-a-prompt",
    "href": "chapter2.html#example-improving-a-prompt",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.9 Example: Improving a Prompt",
    "text": "4.9 Example: Improving a Prompt\n\n❌ “Tell me about climate change.”\n✅ “Summarize the latest peer-reviewed research on climate change and its impact on global food production.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#managing-prompts-effectively",
    "href": "chapter2.html#managing-prompts-effectively",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.10 Managing Prompts Effectively",
    "text": "4.10 Managing Prompts Effectively\n\n\nMaintain a personal prompt library.\nTrack version history for improved iterations.\nDesign role-based prompts for AI personalization.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#evaluating-ai-responses",
    "href": "chapter2.html#evaluating-ai-responses",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.11 Evaluating AI Responses",
    "text": "4.11 Evaluating AI Responses\n\n\n\n\nCriteria\nDescription\n\n\n\n\nAccuracy\nDoes the response match facts?\n\n\nClarity\nIs the output understandable and well-structured?\n\n\nRelevance\nDoes it meet the intended request?\n\n\nBias Check\nDoes the AI introduce misinformation or bias?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#thought-exercise-understanding-prompt-structure",
    "href": "chapter2.html#thought-exercise-understanding-prompt-structure",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.12 Thought Exercise: Understanding Prompt Structure",
    "text": "4.12 Thought Exercise: Understanding Prompt Structure\nTask: Compare good vs. bad prompts\n\n\nWhy do vague prompts generate lower-quality responses?\nHow does adding context and constraints improve AI output?\nCan AI be manipulated with different prompts?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#hands-on-improving-basic-prompts",
    "href": "chapter2.html#hands-on-improving-basic-prompts",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.13 Hands-On: Improving Basic Prompts",
    "text": "4.13 Hands-On: Improving Basic Prompts\nExample Task:\n\n\nBasic: “Explain economics.”\nImproved: “Explain the key principles of macroeconomics with examples from the 21st century.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#hands-on-role-based-prompting",
    "href": "chapter2.html#hands-on-role-based-prompting",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.14 Hands-On: Role-Based Prompting",
    "text": "4.14 Hands-On: Role-Based Prompting\n\n\n“You are an AI trained in political science. Explain the impact of AI on democracy.”\n“You are a historian in the year 2050. Reflect on how AI changed society.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#hands-on-ai-assisted-research",
    "href": "chapter2.html#hands-on-ai-assisted-research",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.15 Hands-On: AI-Assisted Research",
    "text": "4.15 Hands-On: AI-Assisted Research\nTools to Use:\n\n\nGPT-4, Claude 2, DeepSeek R1\nTasks:\n\nSummarize a research paper.\n\nGenerate a structured literature review.\n\nIdentify missing data in a dataset.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter2.html#visualizing-model-performance-linear-regression-vs.-random-forest",
    "href": "chapter2.html#visualizing-model-performance-linear-regression-vs.-random-forest",
    "title": "3  Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration",
    "section": "4.4 Visualizing Model Performance: Linear Regression vs. Random Forest",
    "text": "4.4 Visualizing Model Performance: Linear Regression vs. Random Forest\nHow to tell which is better? Compares two key metrics: RMSE (Root Mean Squared Error) and R-squared, including error bars to reflect variability across cross-validation folds.\n\nBoxplots for RMSE and R-squared\n\n\n\nShow the code\nlibrary(caret)\nlibrary(randomForest)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\ndata(Boston, package = \"MASS\")\nset.seed(123)\ntrain_index &lt;- createDataPartition(Boston$medv, p = 0.7, list = FALSE)\ntrain_data &lt;- Boston[train_index, ]\ntest_data &lt;- Boston[-train_index, ]\nctrl &lt;- trainControl(method = \"cv\", number = 5)\nlm_model &lt;- train(medv ~ ., data = train_data, method = \"lm\", trControl = ctrl)\nrf_model &lt;- train(medv ~ ., data = train_data, method = \"rf\", trControl = ctrl)\nresults &lt;- resamples(list(Linear = lm_model, RandomForest = rf_model))\n# Convert results to a tidy format for ggplot2\nresults_df &lt;- as.data.frame(results$values)\n\nresults_long &lt;- results_df %&gt;%\n  pivot_longer(\n    cols = -Resample,  # Exclude the 'Resample' column\n    names_to = c(\"Model\", \"Metric\"),\n    names_sep = \"~\",\n    values_to = \"Value\"\n  )\n\n\n# Filter for RMSE and R-squared only\nresults_long &lt;- subset(results_long, Metric %in% c(\"RMSE\", \"Rsquared\"))\n\n# Plot\nggplot(results_long, aes(x = Model, y = Value, fill = Model)) +\n  geom_boxplot(alpha = 0.7) +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(\n    title = \"Model Performance Comparison: Linear Regression vs Random Forest\",\n    x = \"Model\",\n    y = \"Metric Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n4.4.0.1 b. Bar Chart with Error Bars\n\n\nShow the code\n# Calculate means and standard deviations for each metric/model\nlibrary(dplyr)\nsummary_stats &lt;- results_long %&gt;%\n  group_by(Metric, Model) %&gt;%\n  summarize(\n    Mean = mean(Value, na.rm = TRUE),\n    SD = sd(Value, na.rm = TRUE)\n  )\n\n# Plot bar chart with error bars\nggplot(summary_stats, aes(x = Model, y = Mean, fill = Model)) +\n  geom_col(position = \"dodge\", width = 0.6, alpha = 0.8) +\n  geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2) +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(\n    title = \"Mean Performance with Standard Deviation\",\n    x = \"Model\",\n    y = \"Mean Value\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n4.4.1 How to Interpret the Plots\n\nBoxplots show the distribution of RMSE and R-squared across cross-validation folds for each model.\n\nLower RMSE and higher R-squared indicate better performance.\nRandom Forest typically shows lower RMSE and higher R-squared than Linear Regression, reflecting better predictive accuracy.\n\nBar charts with error bars provide a summary view of the mean and variability for each metric and model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2: Fundamentals - Data Visualization, Machine Learning, and GenAI Integration</span>"
    ]
  },
  {
    "objectID": "chapter3.html#why-data-science-with-genai",
    "href": "chapter3.html#why-data-science-with-genai",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "4.2 3.1 Why Data Science with GenAI",
    "text": "4.2 3.1 Why Data Science with GenAI\nGenAI fundamentally transforms data science by automating routine tasks, enhancing analytical capabilities, and democratizing access to advanced analytics.\n\n4.2.1 Key Advantages\n\nAutomation: Tasks like data cleaning, preprocessing, exploratory analysis, and code generation are streamlined, saving time and reducing manual errors.\nEnhanced Analytics: Natural language interfaces enable intuitive data querying and insight generation, while multi-modal analysis brings together text, images, and structured data.\nDemocratization: Non-technical users can interact with data and models using plain language, lowering barriers to entry and enabling broader participation in data-driven work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter3.html#large-language-models-and-ai-models-overview",
    "href": "chapter3.html#large-language-models-and-ai-models-overview",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "4.3 3.2 Large Language Models and AI Models Overview",
    "text": "4.3 3.2 Large Language Models and AI Models Overview\nLarge Language Models (LLMs) such as GPT-4, Claude, and others have revolutionized how we interact with data and code.\n\n4.3.1 Capabilities in Data Science\n\nText Analysis & Generation: LLMs can perform sentiment analysis, summarize reports, and generate code documentation.\nData Understanding: They recognize patterns, suggest features, and assess data quality in both structured and unstructured formats.\nAnalytical Assistance: LLMs can recommend statistical methods, interpret models, and even generate hypotheses from data trends.\n\n\n\n4.3.2 Sample Code Exercise: Simulating LLM-Assisted Data Analysis\n\n\nShow the code\nlibrary(dplyr)\nlibrary(ggplot2)\n\nllm_data_assistant &lt;- function(data, user_query) {\n  data_profile &lt;- list(\n    variables = names(data),\n    n_observations = nrow(data),\n    missing_values = sapply(data, function(x) sum(is.na(x))),\n    data_types = sapply(data, class),\n    suggested_target = names(data)[ncol(data)]\n  )\n  if (grepl(\"correlation|relationship\", user_query, ignore.case = TRUE)) {\n    suggestion &lt;- \"correlation_analysis\"\n    code_template &lt;- \"\n# Correlation analysis\ncorrelation_matrix &lt;- cor(numeric_data, use = 'complete.obs')\ncorrplot::corrplot(correlation_matrix, method = 'circle')\n\"\n  } else if (grepl(\"distribution|summary\", user_query, ignore.case = TRUE)) {\n    suggestion &lt;- \"distribution_analysis\"\n    code_template &lt;- \"\n# Distribution analysis\ndata %&gt;%\n  select_if(is.numeric) %&gt;%\n  gather(key = 'variable', value = 'value') %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~variable, scales = 'free')\n\"\n  } else {\n    suggestion &lt;- \"exploratory_analysis\"\n    code_template &lt;- \"\n# Exploratory data analysis\nsummary(data)\n\"\n  }\n  return(list(\n    profile = data_profile,\n    suggestion = suggestion,\n    code = code_template,\n    explanation = paste(\"Based on your query:\", user_query,\n                        \"I recommend starting with\", suggestion)\n  ))\n}\n\nuser_query &lt;- \"I want to understand the relationships between variables\"\nassistance &lt;- llm_data_assistant(mtcars, user_query)\ncat(assistance$explanation)\n\n\nBased on your query: I want to understand the relationships between variables I recommend starting with correlation_analysis\n\n\nShow the code\ncat(assistance$code)\n\n\n\n# Correlation analysis\ncorrelation_matrix &lt;- cor(numeric_data, use = 'complete.obs')\ncorrplot::corrplot(correlation_matrix, method = 'circle')\n\n\nInterpretation of Output:\n\nThe function profiles the dataset, identifying variables, missing values, and data types.\nIt analyzes the user’s query to recommend an appropriate analysis (e.g., correlation analysis for relationships).\nIt generates a code template for the suggested analysis and explains the reasoning.\nThis simulates how an LLM can guide a user from question to actionable code, accelerating exploratory data analysis and reducing technical barriers.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter3.html#vibe-coding-and-prompt-engineering",
    "href": "chapter3.html#vibe-coding-and-prompt-engineering",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "4.4 3.3 Vibe Coding and Prompt Engineering",
    "text": "4.4 3.3 Vibe Coding and Prompt Engineering\n\n4.4.1 What is Vibe Coding?\nVibe coding is an AI-driven programming paradigm where the user describes a data or coding problem in natural language, and the AI generates or iteratively refines the code. The concept is widely attributed to Andrej Karpathy, who popularized the term in 2023–2024, and the Graphite team, who formalized it as a collaborative, conversational approach to software development.\n\n4.4.1.1 Advantages\n\nRapid Prototyping: Quickly generate working code from high-level descriptions.\nLower Entry Barriers: Non-experts can create complex analyses without deep syntax knowledge.\nCreativity and Exploration: Encourages experimentation and creative problem-solving.\nIterative Refinement: Code evolves through a conversational loop, improving with each prompt.\n\n\n\n4.4.1.2 Disadvantages\n\nLess Determinism: Results may vary based on prompt phrasing or context.\nPotential for Hidden Errors: AI-generated code may work but lack robustness or best practices.\nLimited Deep Customization: Fine-tuning or optimizing complex code may still require expert intervention.\nContext Drift: Without careful prompt management, the AI may lose track of previous context or requirements.\n\n\n\n4.4.1.3 Latest Developments\n\nIntegrated Development Environments (IDEs): Tools like GitHub Copilot, Graphite, and OpenAI Code Interpreter now support vibe coding workflows.\nMulti-modal Vibe Coding: Some platforms incorporate diagrams, tables, or voice as part of the prompt.\nContext Windows: Modern LLMs support larger context windows, enabling richer, more sustained conversations and project memory.\nTeam Collaboration: Vibe coding is being extended to multi-user settings, where teams and AI agents co-create codebases.\n\n\n\n\n4.4.2 Key Characteristics\n\nEmphasis on creative flow and intuition rather than rigid syntax.\nFocus on the vibe (intent, tone, context) of the prompt rather than precise instructions.\nIterative, conversational development: code is refined through back-and-forth interaction.\nHuman-AI partnership: user steers, AI generates and suggests.\n\n\n\n4.4.3 Vibe Coding vs. Traditional Programming\n\n\n\n\n\n\n\n\nAspect\nTraditional Programming\nVibe Coding\n\n\n\n\nApproach\nLine-by-line code writing\nNatural language problem description\n\n\nFocus\nSyntax accuracy and logic\nCreative flow and iteration\n\n\nSkills\nProgramming mastery\nPrompt engineering and AI collaboration\n\n\nOutput\nDeterministic code\nIterative, conversational development\n\n\nError Handling\nDebugging syntax/logic\nRefining prompts and context\n\n\n\n\n\n4.4.4 Sample Vibe Coding Exercise\n\n\nShow the code\n# Vibe coding approach (natural language to AI)\nvibe_prompt &lt;- \"\nCreate a professional scatter plot visualization that shows the relationship\nbetween fuel efficiency and engine power using the mtcars dataset.\nMake it visually appealing with a clean theme, appropriate colors,\nand clear labels that would be suitable for a business presentation.\nAdd a trend line to highlight the relationship pattern.\n\"\n\nvibe_generated_code &lt;- function() {\n  library(ggplot2)\n  library(scales)\n  mtcars %&gt;%\n    ggplot(aes(x = mpg, y = hp)) +\n    geom_point(aes(color = factor(cyl)), alpha = 0.7, size = 3) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"navy\", linetype = \"dashed\") +\n    scale_color_viridis_d(name = \"Cylinders\") +\n    theme_minimal(base_size = 12) +\n    theme(\n      panel.grid.minor = element_blank(),\n      legend.position = \"bottom\",\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      plot.subtitle = element_text(hjust = 0.5, color = \"gray60\")\n    ) +\n    labs(\n      title = \"Engine Power vs Fuel Efficiency\",\n      subtitle = \"Higher fuel efficiency generally correlates with lower engine power\",\n      x = \"Miles per Gallon (MPG)\",\n      y = \"Horsepower (HP)\",\n      caption = \"Data source: 1974 Motor Trend magazine\"\n    )\n}\nvibe_generated_code()\n\n\n\n\n\n\n\n\n\nInterpretation of Output:\n\nThe AI translates a high-level prompt into a polished, multi-layered ggplot2 visualization.\nThe code uses color, size, and trend lines to communicate relationships, echoing best practices in data storytelling.\nThe user can further refine the prompt to adjust style, add annotations, or change data sources, making it a collaborative, iterative process.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter3.html#the-role-of-context-in-genai-and-vibe-coding",
    "href": "chapter3.html#the-role-of-context-in-genai-and-vibe-coding",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "4.5 3.4 The Role of “Context” in GenAI and Vibe Coding",
    "text": "4.5 3.4 The Role of “Context” in GenAI and Vibe Coding\nContext in GenAI refers to all the information the AI uses to generate its response—including the current prompt, previous conversation, user preferences, and any relevant data or tools.\n\n4.5.1 Why Context Matters\n\nRelevance: Context ensures that AI responses are tailored to the user’s current task and prior interactions.\nContinuity: In multi-step workflows, context allows the AI to remember earlier steps, user corrections, or project goals.\nPersonalization: Context enables the AI to adapt its style, terminology, or code conventions to match the user’s needs.\nReduced Ambiguity: More context leads to more accurate, less generic answers.\n\n\n\n4.5.2 How Context is Managed\n\nPrompt Engineering: Users can provide context explicitly (e.g., “You are an R data scientist…”).\nSession Memory: Modern LLMs can retain information across longer conversations.\nExternal Tools: Some platforms integrate with files, APIs, or databases to provide richer context.\nSystem Prompts: Developers can set background instructions or constraints that persist throughout a session.\n\n\n\n4.5.3 Example: Improving Context in Prompts\n\nBasic Prompt:\n“Plot mpg vs hp in mtcars.”\nContextual Prompt:\n“You are an expert in automotive data analysis. Using the mtcars dataset, create a scatter plot of mpg vs hp, color by the number of cylinders, and add a regression line. The plot should be suitable for a business audience and include clear labels.”\n\nThe contextual prompt yields a more relevant, tailored, and high-quality output.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter3.html#collaborative-intelligence-and-human-ai-partnerships",
    "href": "chapter3.html#collaborative-intelligence-and-human-ai-partnerships",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "4.6 3.5 Collaborative Intelligence and Human-AI Partnerships",
    "text": "4.6 3.5 Collaborative Intelligence and Human-AI Partnerships\nCollaborative intelligence is the practice of humans and AI working together—each contributing strengths to achieve superior results.\n\n4.6.1 Core Components\n\nSustained Communication: Ongoing, iterative dialogue.\nShared Objectives: Alignment on goals and outcomes.\nAdaptive Understanding: AI adjusts to changing user needs and context.\nPerformance Enhancement: Combining human creativity with AI speed and scale.\nExperience Improvement: Making data science more accessible, efficient, and engaging.\n\n\n\n4.6.2 Strategic Applications\n\nResearch Discovery: AI accelerates literature reviews and hypothesis generation.\nData Exploration: AI suggests analytical pathways; humans provide domain expertise.\nModel Development: AI proposes models; humans interpret and refine.\nInsight Generation: AI surfaces patterns; humans contextualize and communicate findings.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter3.html#hands-on-exercise-vibe-coding-and-collaborative-intelligence",
    "href": "chapter3.html#hands-on-exercise-vibe-coding-and-collaborative-intelligence",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "4.7 Hands-on Exercise: Vibe Coding and Collaborative Intelligence",
    "text": "4.7 Hands-on Exercise: Vibe Coding and Collaborative Intelligence\nChallenge:\n- Use natural language prompts to generate analysis code. - Refine outputs through conversational feedback. - Combine AI-generated insights with human domain knowledge. - Compare vibe coding with traditional programming approaches. - Reflect on the collaborative process and lessons learned.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter3.html#references",
    "href": "chapter3.html#references",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "4.8 References",
    "text": "4.8 References\nAisera. 2025. “What is Human AI Collaboration?” Aisera Blog, May 29.\nBito. 2025. “AI Documentation Generator.” Bito AI.\nClanX. 2024. “Human-AI Collaboration: What it is and Why it Matters?” ClanX Blog, January 26.\nDallas Data Science Academy. 2025. “Integrating Generative AI in Data Science Projects.” Dallas Data Science Academy Blog, January 6.\nDavenport, Thomas H., Nitin Mittal, and Ilya Goldin. 2024. “The Impact of Generative AI on Data Science.” DATAVERSITY, May 1.\nDecodo. 2025. “How to Use LLM for Data Analysis: A Comprehensive Guide.” Decodo Blog, April 15.\nGoodale, Tom. 2025. “Using Generative AI to Help with Statistical Test Selection and Analysis.” MSOR Connections 22(3): 10-17.\nGraphite. 2025. “Understanding ‘Vibe Coding,’ the Future of AI-Driven Development.” Graphite Blog, July 8.\nGrTech. 2025. “What is Vibe Coding? Everything You Need To Know About It.” GrTech Blog, April 21.\nIBM. 2025. “What is Vibe Coding?” IBM Think Topics, April 8.\nInfoRes. 2025. “The Rise of Collaborative Intelligence: Human-AI Partnership in Academic Research.” Information Research Communications 1(2): 161-163.\nKarpathy, Andrej. 2025. “Vibe Coding.” Wikipedia, March 3.\nLakera AI. 2025. “The Ultimate Guide to Prompt Engineering in 2025.” Lakera AI Blog, May 21.\nMonterail. 2023. “AI-Powered Coding Assistants: Best Practices to Boost Software Development.” Monterail Blog, September 11.\nPromptHub. 2025. “10 Best Practices for Prompt Engineering with Any Model.” PromptHub Blog, March 5.\nPrompting Guide. 2025. “General Tips for Designing Prompts.” Prompt Engineering Guide, June 7.\nSecureWorld. 2024. “Human-AI Teaming in the Age of Collaborative Intelligence.” SecureWorld, November 26.\nSlack. 2024. “Collaborative Intelligence: People and AI Working Smarter Together.” Slack Blog, January 15.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter3.html#learning-objectives",
    "href": "chapter3.html#learning-objectives",
    "title": "4  Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence",
    "section": "",
    "text": "Understand the rationale for integrating GenAI with data science\nExplore Large Language Models and their capabilities\nMaster vibe coding and prompt engineering techniques\nDevelop collaborative intelligence skills for human-AI partnerships",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3: GenAI Applications - LLMs, Vibe Coding, and Collaborative Intelligence</span>"
    ]
  },
  {
    "objectID": "chapter4.html#retrieval-augmented-generation-rag-concept-history-and-advantages",
    "href": "chapter4.html#retrieval-augmented-generation-rag-concept-history-and-advantages",
    "title": "5  Chapter 4: Advanced Applications – RAG, MCP, and Future Trends",
    "section": "5.2 4.1 Retrieval-Augmented Generation (RAG): Concept, History, and Advantages",
    "text": "5.2 4.1 Retrieval-Augmented Generation (RAG): Concept, History, and Advantages\n\n5.2.1 What is RAG?\nRetrieval-Augmented Generation (RAG) is an architecture that combines large language models (LLMs) with external information retrieval systems. Instead of relying solely on a model’s static internal knowledge, RAG dynamically retrieves relevant documents or facts from external sources and feeds them into the LLM as context for response generation.\n\n\n5.2.2 Why Was RAG Developed?\n\nStatic Training Data Limitation: Traditional LLMs are trained on a fixed dataset and cannot access new information after training. Their knowledge is “frozen” at the time of their last update.\nHallucination and Factuality: LLMs often generate plausible-sounding but incorrect or outdated information, especially for recent events or niche topics.\nRAG’s Solution: By retrieving up-to-date and domain-specific information at inference time, RAG systems can ground their responses in real evidence, improve factual accuracy, and reduce hallucinations.\n\n\n\n5.2.3 How RAG Works: Flowchart\n\n\nShow the code\nlibrary(DiagrammeR)\n\ngrViz(\"\ndigraph LLM_Pipeline {\n  graph [rankdir = TB, layout = dot]\n\n  node [shape = box, style = filled, fillcolor = lightblue, fontname = Helvetica]\n\n  A [label = 'User Query']\n  B [label = 'Query Processing']\n  C [label = 'Information Retrieval\\\\n(search external sources)']\n  D [label = 'Select Top-K Relevant Documents']\n  E [label = 'Context Augmentation\\\\n(add retrieved docs to prompt)']\n  F [label = 'LLM Generation\\\\n(generate answer using context)']\n  G [label = 'Response to User']\n\n  A -&gt; B -&gt; C -&gt; D -&gt; E -&gt; F -&gt; G\n}\n\")\n\n\n\n\n\n\nExplanation of Steps:\n\nUser Query: The user asks a question or makes a request.\nQuery Processing: The system reformulates the query for retrieval.\nInformation Retrieval: The system searches external sources.\nSelect Top-K Documents: The most relevant documents are selected.\nContext Augmentation: These documents are added to the LLM’s input prompt.\nLLM Generation: The LLM generates a response, using both its internal knowledge and the retrieved context.\nResponse to User: The grounded, evidence-based answer is returned.\n\n\n\n5.2.4 Why is RAG Better Than Static LLMs?\n\nDynamic Knowledge: RAG can access the latest information, overcoming the “knowledge cut-off” of static LLMs.\nReduced Hallucination: By grounding answers in retrieved evidence, RAG mitigates the risk of making up facts.\nDomain Adaptability: RAG can specialize in any domain by connecting to relevant databases or corpora, without retraining the LLM.\nTransparency: RAG can cite sources, increasing trust and interpretability.\n\n\n\n5.2.5 Example: RAG in Practice\nSuppose a user asks:\n“What are the latest COVID-19 statistics in Taiwan?”\n\nA static LLM (trained in 2023) cannot answer accurately.\nA RAG system retrieves the latest statistics from a government database or news API, includes them in the prompt, and the LLM summarizes and communicates the answer.\n\nBenefits of RAG (Lewis et al. 2020; Pinecone 2024):\n\nEnhanced accuracy: Up-to-date, factual information from external sources\nReduced hallucinations: Grounding responses in verified information\nDomain customization: Integration with specialized knowledge bases\nTransparency: Ability to cite sources and provide evidence\n\n\n5.2.5.1 Sample RAG Implementation\n\n\nShow the code\n# Simulating RAG System for Data Science Knowledge\n\nlibrary(dplyr)\nlibrary(stringr)\n\n# Create a simple knowledge base\ndata_science_kb &lt;- data.frame(\n  document_id = 1:10,\n  content = c(\n    \"Cross-validation is essential for model evaluation to prevent overfitting\",\n    \"Feature engineering involves creating new variables from existing data\",\n    \"The bias-variance tradeoff is fundamental to machine learning model performance\",\n    \"Exploratory data analysis should always precede model building\",\n    \"Data cleaning typically consumes 80% of a data scientist's time\",\n    \"Correlation does not imply causation in statistical analysis\",\n    \"Ensemble methods often outperform single models in predictive accuracy\",\n    \"Data visualization is crucial for communicating insights effectively\",\n    \"Missing data imputation requires careful consideration of the missingness mechanism\",\n    \"Statistical significance does not always indicate practical significance\"\n  ),\n  keywords = c(\n    \"cross-validation, model evaluation, overfitting\",\n    \"feature engineering, variables, data transformation\",\n    \"bias-variance tradeoff, machine learning, model performance\",\n    \"exploratory data analysis, EDA, model building\",\n    \"data cleaning, preprocessing, time management\",\n    \"correlation, causation, statistical analysis\",\n    \"ensemble methods, model performance, accuracy\",\n    \"data visualization, communication, insights\",\n    \"missing data, imputation, missingness\",\n    \"statistical significance, practical significance, interpretation\"\n  ),\n  relevance_score = runif(10, 0.7, 1.0)\n)\n\n# RAG retrieval function\nrag_retrieval &lt;- function(query, knowledge_base, top_k = 3) {\n  query_words &lt;- tolower(unlist(strsplit(query, \"\\\\s+\")))\n  kb_with_scores &lt;- knowledge_base %&gt;%\n    rowwise() %&gt;%\n    mutate(\n      query_relevance = sum(sapply(query_words, function(word) {\n        grepl(word, tolower(content)) + grepl(word, tolower(keywords))\n      }))\n    ) %&gt;%\n    arrange(desc(query_relevance)) %&gt;%\n    slice_head(n = top_k)\n  return(kb_with_scores)\n}\n\n# RAG generation function\nrag_generate_response &lt;- function(query, retrieved_docs) {\n  context &lt;- paste(retrieved_docs$content, collapse = \" \")\n  response &lt;- paste0(\n    \"Based on the retrieved knowledge: \",\n    \"\\n\\nQuery: \", query,\n    \"\\n\\nRelevant Information:\\n\",\n    paste(retrieved_docs$content, collapse = \"\\n\"),\n    \"\\n\\nSynthesized Response: \",\n    \"The retrieved documents indicate that \", \n    tolower(substr(query, 1, nchar(query)-1)),\n    \" involves multiple considerations from the data science literature.\"\n  )\n  return(list(\n    response = response,\n    sources = retrieved_docs$document_id,\n    context_used = context\n  ))\n}\n\n# Demonstrate RAG system\nuser_query &lt;- \"How should I evaluate my machine learning model?\"\nretrieved &lt;- rag_retrieval(user_query, data_science_kb)\nrag_response &lt;- rag_generate_response(user_query, retrieved)\n\ncat(\"RAG System Response:\\n\")\n\n\nRAG System Response:\n\n\nShow the code\ncat(rag_response$response)\n\n\nBased on the retrieved knowledge: \n\nQuery: How should I evaluate my machine learning model?\n\nRelevant Information:\nThe bias-variance tradeoff is fundamental to machine learning model performance\nExploratory data analysis should always precede model building\nCross-validation is essential for model evaluation to prevent overfitting\nEnsemble methods often outperform single models in predictive accuracy\nFeature engineering involves creating new variables from existing data\nData cleaning typically consumes 80% of a data scientist's time\nCorrelation does not imply causation in statistical analysis\nData visualization is crucial for communicating insights effectively\nMissing data imputation requires careful consideration of the missingness mechanism\nStatistical significance does not always indicate practical significance\n\nSynthesized Response: The retrieved documents indicate that how should i evaluate my machine learning model involves multiple considerations from the data science literature.\n\n\n\n\n\n5.2.6 Sample RAG Implementation: Interpretation\nThe sample RAG code demonstrates:\n\nCreating a mini knowledge base (data frame of facts and keywords).\nRetrieving the top relevant documents using keyword matching.\nSynthesizing a response by combining the user query and retrieved facts.\nThe output is a context-aware answer, referencing the specific documents used.\n\nInterpretation:\nThis approach shows how RAG systems can “look up” supporting information at runtime, ensuring answers are up-to-date and verifiable—something static LLMs cannot do.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications – RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter4.html#model-context-protocol-mcp-impact-and-innovation",
    "href": "chapter4.html#model-context-protocol-mcp-impact-and-innovation",
    "title": "5  Chapter 4: Advanced Applications – RAG, MCP, and Future Trends",
    "section": "5.3 4.2 Model Context Protocol (MCP): Impact and Innovation",
    "text": "5.3 4.2 Model Context Protocol (MCP): Impact and Innovation\n\n5.3.1 What is MCP?\nModel Context Protocol (MCP) is a standard that enables LLMs to interact with external tools, APIs, and data sources in a structured, programmatic way. MCP defines how an LLM can call functions, query databases, or use plugins during a conversation.\n\n\n5.3.2 What Impact Does MCP Have?\n\nTool Use: LLMs can perform actions (e.g., run code, fetch live data, manipulate files) rather than just generate text.\nWorkflow Automation: Enables AI agents to orchestrate complex tasks, integrating multiple tools and data sources.\nInteractivity: Users can have more dynamic, actionable conversations (e.g., “Plot this data and email me the chart”).\nEcosystem Expansion: MCP allows developers to build new tools and services that plug directly into LLM workflows.\n\n\n\n5.3.3 What New Capabilities Does MCP Bring?\n\nBeyond Text Generation: LLMs become “doers” as well as “talkers”—they can execute, retrieve, and process information in real time.\nPersonalization: AI can access user files, calendars, or preferences to tailor responses and actions.\nReliability: By delegating specialized tasks to external tools (e.g., a calculator for math), LLMs can provide more accurate and robust outputs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications – RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter4.html#synthetic-data-silicon-sampling-and-beyond",
    "href": "chapter4.html#synthetic-data-silicon-sampling-and-beyond",
    "title": "5  Chapter 4: Advanced Applications – RAG, MCP, and Future Trends",
    "section": "5.4 4.3 Synthetic Data: Silicon Sampling and Beyond",
    "text": "5.4 4.3 Synthetic Data: Silicon Sampling and Beyond\n\n5.4.1 What is Synthetic Data?\nSynthetic data refers to artificially generated data that mimics the statistical properties of real datasets. It is used for privacy protection, testing, simulation, and augmenting training data.\n\n\n5.4.2 Silicon Sampling (Argyle et al. 2023)\nSilicon sampling is a method for generating synthetic survey data using LLMs or similar generative models. The process involves:\n\nTraining a model on real survey data.\nGenerating new, realistic individual-level responses that preserve the joint distribution of variables.\nValidating the synthetic data to ensure it matches key properties of the original.\n\nAdvantages: - Enables sharing and analysis of sensitive data without privacy risks. - Supports simulation studies for policy analysis or experimental design.\nReference:\nArgyle, Lisa P., Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and David Wingate. “Out of one, many: Using language models to simulate human samples.” Political Analysis 31, no. 3 (2023): 337-351.\n\n\n5.4.3 Agent-Based Modeling, AI Agents, and Agentic AI: What’s the Difference?\n\n\n\n\n\n\n\n\nConcept\nDescription\nExample Use Case\n\n\n\n\nAgent-Based Modeling\nSimulation approach where many autonomous “agents” interact based on simple rules.\nModeling voter behavior in elections\n\n\nAI Agent\nA software entity that can perceive, reason, and act autonomously in a digital environment.\nChatbot that schedules meetings\n\n\nAgentic AI\nAdvanced AI systems capable of pursuing goals, adapting, and collaborating with humans\nAutonomous research assistant that plans, learns, and executes tasks\n\n\n\nKey Differences:\n\nAgent-Based Modeling is a simulation technique in social science and complex systems.\nAI Agents are practical software tools that perform tasks (can be simple or complex).\nAgentic AI refers to emerging, more autonomous and adaptive AI systems that can set and pursue their own objectives, often with multi-step reasoning and collaboration.\n\n\n5.4.3.1 Sample Synthetic Data Generation\n\n\nShow the code\n# Synthetic Data Generation for Data Science Training\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\ngenerate_synthetic_customers &lt;- function(n = 1000, seed = 123) {\n  set.seed(seed)\n  correlation_matrix &lt;- matrix(c(\n    1.0, 0.6, -0.4, 0.3,\n    0.6, 1.0, -0.2, 0.5,\n    -0.4, -0.2, 1.0, -0.1,\n    0.3, 0.5, -0.1, 1.0\n  ), nrow = 4)\n  base_vars &lt;- mvrnorm(n, mu = c(35, 50000, 25, 3), Sigma = correlation_matrix * 100)\n  synthetic_data &lt;- data.frame(\n    customer_id = 1:n,\n    age = pmax(18, pmin(80, round(base_vars[,1]))),\n    income = pmax(20000, round(base_vars[,2])),\n    experience_years = pmax(0, pmin(40, round(base_vars[,3]))),\n    satisfaction_score = pmax(1, pmin(5, round(base_vars[,4])))\n  ) %&gt;%\n    mutate(\n      churn_probability = plogis(-2 + 0.02*age - 0.00003*income + \n                                 0.05*experience_years - 0.8*satisfaction_score + \n                                 rnorm(n, 0, 0.5)),\n      churn = rbinom(n, 1, churn_probability),\n      segment = case_when(\n        income &gt; 70000 & satisfaction_score &gt;= 4 ~ \"Premium\",\n        income &gt; 45000 & satisfaction_score &gt;= 3 ~ \"Standard\",\n        TRUE ~ \"Basic\"\n      )\n    ) \n  return(synthetic_data)\n}\n\nsynthetic_customers &lt;- generate_synthetic_customers(1000)\ncat(\"Synthetic Data Summary:\\n\")\n\n\nSynthetic Data Summary:\n\n\nShow the code\nsummary(synthetic_customers)\n\n\n  customer_id          age            income      experience_years\n Min.   :   1.0   Min.   :18.00   Min.   :49975   Min.   : 0.00   \n 1st Qu.: 250.8   1st Qu.:29.00   1st Qu.:49994   1st Qu.:18.00   \n Median : 500.5   Median :35.00   Median :50000   Median :25.00   \n Mean   : 500.5   Mean   :35.42   Mean   :50000   Mean   :24.43   \n 3rd Qu.: 750.2   3rd Qu.:42.00   3rd Qu.:50007   3rd Qu.:32.00   \n Max.   :1000.0   Max.   :67.00   Max.   :50032   Max.   :40.00   \n satisfaction_score churn_probability       churn         segment         \n Min.   :1.000      Min.   :0.0006705   Min.   :0.000   Length:1000       \n 1st Qu.:1.000      1st Qu.:0.0041187   1st Qu.:0.000   Class :character  \n Median :3.000      Median :0.0194273   Median :0.000   Mode  :character  \n Mean   :2.996      Mean   :0.0482846   Mean   :0.049                     \n 3rd Qu.:5.000      3rd Qu.:0.0817124   3rd Qu.:0.000                     \n Max.   :5.000      Max.   :0.2999137   Max.   :1.000                     \n\n\nShow the code\nggplot(synthetic_customers, aes(x = income, y = satisfaction_score, color = factor(churn))) +\n  geom_point(alpha = 0.6) +\n  facet_wrap(~segment) +\n  theme_minimal() +\n  labs(title = \"Synthetic Customer Data Validation\",\n       subtitle = \"Checking realistic relationships between variables\",\n       color = \"Churn\")\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.4 4.4 Context Engineering vs. Prompt Engineering\nThe evolution from prompt engineering to context engineering represents a fundamental shift in AI interaction paradigms (Lakera AI 2025; Contextual AI 2024; Prompting Guide 2025).\n\n5.4.4.1 Context Engineering Definition (Lakera AI 2025; Contextual AI 2024)\nContext engineering is the discipline of designing and building dynamic systems that provide the right information and tools, in the right format, at the right time, to give an LLM everything it needs to accomplish a task (Lakera AI 2025).\n\n\n5.4.4.2 Key Differences (Lakera AI 2025; Contextual AI 2024; Prompting Guide 2025)\n\n\n\n\n\n\n\n\nAspect\nPrompt Engineering\nContext Engineering\n\n\n\n\nScope\nSingle prompt optimization\nSystem-level design\n\n\nFocus\nHow to ask\nWhat information to provide\n\n\nApproach\nString crafting\nInformation architecture\n\n\nContext Window\nLimited consideration\nCentral design consideration\n\n\nPersistence\nStateless interactions\nStateful system design\n\n\nComplexity\nSimple to moderate\nSystem-level complexity\n\n\n\n\n\n5.4.4.3 Context Engineering Components (Lakera AI 2025; Prompting Guide 2025)\n\nInformation Architecture:\n\nInstructions/System prompts\nUser input and queries\nShort-term memory (conversation history)\nLong-term memory (persistent knowledge)\nRetrieved information (RAG systems)\nAvailable tools and their definitions\nStructured output specifications\n\nDynamic Context Management:\n\nReal-time context optimization\nToken limit management\nInformation prioritization\nContext compression techniques\n\n\n\n\n\n5.4.5 4.5 Future Trends in GenAI for Data Science\n\n5.4.5.1 Emerging Paradigms (Anthropic 2024; Bito 2025; Dallas Data Science Academy 2025)\n1. Agentic AI Systems (Anthropic 2024)\n\nAutonomous AI agents for complex data science workflows\nMulti-agent collaboration for large-scale analysis projects\nAdaptive learning and improvement capabilities\n\n2. Multimodal Integration (Monterail 2023)\n\nIntegration of text, images, audio, and structured data analysis\nCross-modal insight generation and validation\nUnified analytical frameworks for diverse data types\n\n3. Democratization Acceleration (Bito 2025; Aisera 2025)\n\nNatural language programming interfaces becoming mainstream\nReduced technical barriers for non-expert users\nAutomated ML pipeline generation and optimization\n\n4. Ethical AI and Responsible Development (Bito 2025)\n\nEnhanced bias detection and mitigation systems\nTransparent AI decision-making processes\nResponsible AI frameworks for data science applications",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications – RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter4.html#summary-table-key-innovations",
    "href": "chapter4.html#summary-table-key-innovations",
    "title": "5  Chapter 4: Advanced Applications – RAG, MCP, and Future Trends",
    "section": "5.5 Summary Table: Key Innovations",
    "text": "5.5 Summary Table: Key Innovations\n\n\n\n\n\n\n\n\nTechnology\nWhat It Does\nWhy It Matters\n\n\n\n\nRAG\nBrings live, retrieved info to LLMs\nUp-to-date, grounded, transparent\n\n\nMCP\nLets LLMs use external tools & APIs\nActionable, interactive, extensible\n\n\nSilicon Sampling\nGenerates privacy-preserving synthetic data\nEnables open research, simulation\n\n\nAgentic AI\nAutonomous, goal-driven AI agents\nNext-gen AI with planning/adaptation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4: Advanced Applications – RAG, MCP, and Future Trends</span>"
    ]
  },
  {
    "objectID": "chapter1.html#learning-objectives",
    "href": "chapter1.html#learning-objectives",
    "title": "2  Chapter 1: Data Science Theory and Fundamentals",
    "section": "",
    "text": "Understand fundamental data theory and data generation processes\nDistinguish between Breiman’s “two cultures” of statistical modeling\nExplore Daoud and Dubhashi’s “three cultures” framework\nDifferentiate between big data, small data, found data, and made data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1: Data Science Theory and Fundamentals</span>"
    ]
  }
]