[
["index.html", "Data Science in Practice Chapter 1 Prerequisites", " Data Science in Practice Karl Ho 2020-07-27 Chapter 1 Prerequisites This course requires no prior experience in data programming. Yet, if you have some programming experience (e.g. SPSS, Stata, HTML), it will be helpful. R and Python will be the main language used in this course. Students are encouraged to install the open-sourced software on own computer on MacOS, Linux or Windows operating systems. Mobile operating systems are not supported. All packages and accounts are free and supported by open sources. For class workshops, users need to have a Google and/or GitHub account for logging into RStudio Cloud and Google Colab. Recommended software and IDE’s: R version 4.x (https://cran.r-project.org) RStudio version 1.3.x (https://www.rstudio.com) Optional software and IDE’s *: Anaconda 3 version 1.9.12 (https://www.anaconda.com)* Text editor of own choice (e.g. Atom, Sublime Text, Ultraedit) Cloud websites/accounts: RStudio Cloud (https://rstudio.cloud) Google Colab (https://colab.research.google.com/notebooks/intro.ipynb) (*) – Python 3.x only. "],
["intro.html", "Chapter 2 Introduction 2.1 What is data? 2.2 What is big data? 2.3 Illustration: The story of Google Flu Trend", " Chapter 2 Introduction This course is designed to introduce to students the fundamentals of Data Science with a focus on programming training. The four-day course include lectures focused on the scope and applications of Data Science, followed by modules training students in data programming. There is no prerequisite but the students are expected to bring own device (i.e. PC or Mac computers) and get hands-on in computer programming. Sample programs and instruction materials will be provided and students are recommended to install software ahead of class per instructions. The four-day workshop introduces the four key areas in Data science: Data collection Data visualization Data management Data modeling (machine learning) Let’s first explore the general theory of data. 2.1 What is data? Kinds of Data Quantitative vs. Qualitative Structured vs. Semi-/unstructured Measurement Nominal Ordinal Interval Ratio 2.2 What is big data? The Big data is about data that has huge volume, cannot be on one computer. Has a lot of variety in data types, locations, formats and form. It is also getting created very very fast (velocity) (Doug Laney 2001). According Burt Monroe (2012), the new 5Vs of Big data include: Volume Variety Velocity Vinculation Validity Vinculation means “binding together”, indicating emphasizes interdependent nature or “networkedness” of social data and big data. Validity or veracity refers to the quality of the data and how relevant big data is to the question in interest. In particular, how one can draw inference from big data. Some researchers would also add “value” to emphasize whether the data provide insight instead of confusion to understanding the problem(s). 2.3 Illustration: The story of Google Flu Trend By using Big Data of search queries, Google Flu Trend (GFT) predicted the flu-like illness rate in a population. The findings were published in the one of the top science journals Nature in 2008. However, shortly GFT failed and missed at the peak of the 2013 flu season by 140 percent. Lazer, Kennedy, King and Vespignani (2014) took up the scrutiny and identified the problem. They suggested: \" “Traditional ‘small data’ often offer information that is not contained (or containable) in big data”, and “by combining GFT and lagged [traditional] CDC data, as well as dynamically recalibrating GFT… one can substantially improve on the performance of GFT or the CDC alone.” (Lazer et al. 2014 Science) Lesson learnt: Google should have highest power in data access but it still fails. Size still matters? Yes, but not first. "],
["Datacollection.html", "Chapter 3 Data Collection 3.1 Data Production and Collection 3.2 Data collection showcase: Twitter data", " Chapter 3 Data Collection 3.1 Data Production and Collection Data can be classified by generation methods into two types: made data and found data. These methods include: Survey Experiments Qualitative Data Text Data Web Data Machine Data Complex Data Network Data Multiple-source linked Data The first three methods are considered made or produced data, which go through a well-thought out design process, and data are generated by an instrument or human interviews. These methods usually go through a sampling or pre-selection mechanism, in order that the data will represent the population to a certain extent. The remaining methods are called found or collected data that are primarily extracted from other sources without an intrument or a design process. The sampling mechanism is not pre-designed and usually subject to the property of the extraction algorithm. Some researchers called the former type or made data as small data and the latter the big data. Statistian Leo Breiman (2001) describe these two types of data as two cultures of statistical modeling. 3.2 Data collection showcase: Twitter data APIs (Application program interface) API provides channels to allow an interaction with, and retrieval of, structured data. Many data companies such as Facebook, Twitter and public and private organizations provide APIs for developers or users to access data directly. API methods however limit users to get the data according to the company’s restrictions. Webscraping this method generally refers to using algorithm or simple program to obtain information directly from web pages. data collected using this methods is generally raw and unstructured, meaning more data curation and clean-up are needed before data can be used. The following chart from Munzert et al. (2015) illustrates the technologies of dealing with web data: Source: Munzert et al. 2015.Automated Data Collection with R: a Practical Guide to Web Scraping and Text Mining Hands-on workshop: collecting Twitter data This workshop uses Python to collect Twitter data using a package by Jefferson Henrique and Dimtry Mottl. This non-API method scrapes Twitter data based on Twitter search results by parsing the result page with a scroll loader, then calling to a JSON provider (i.e. collect data in json format). While theoretically it can search through oldest tweets and collect data accordingly, the number of variables are limited to the layout of search results. Mac | Windows 3.2.1 Mac Prerequesites: Python3 Bash/terminal command line tool Python pip package installer Illustration using GetOldTweets3 Install Python 3.x (e.g. Anaconda3) and run the following preparation steps (creating virtual environment, install GetOldTweets3 package using pip): python3 -m venv env source ./env/bin/activate python3 -m pip install GetOldTweets3 Alternatively, pip install -e git+https://github.com/Mottl/GetOldTweets3#egg=GetOldTweets3 There are two methods of collecting Twitter data. The GetOldTweets3 command method is recommended since the data collection process can be time-consuming. Examples: ## Keyword search GetOldTweets3 --querysearch &quot;Trump Kim&quot; --since 2018-01-01 --until 2019-01-16 --output trumpkim.csv ## username search with time period and size limit GetOldTweets3 --username &quot;realDonaldTrump&quot; --since 2016-11-01 --until 2019-03-08 --maxtweets 20000 --output rdt_2016_now.csv 3.2.2 Windows The following procedures are for Windows users (Python2.x or Python 3.x): Prerequisites Python installed Install Anaconda Navigator (http://anaconda.com) Install Python3.x from python.org Visit the following github by Nickson Weng and download the Python package Get-Old_Tweet-Modified https://github.com/NicksonWeng/Get-Old-Tweet-Modified Click on the “Clone or Download” green button on right side Download ZIP to local folder (e.g. c:\\Twitterdata) Unzip the files to the folder Open a terminal windows by typing terminal in the “Type here to search” box. Choose the Command Prompt App Change directory to c:\\Twitterdata Type: pip install -r requirements.txt Perform search using the following criteria (username or keyword) Examples: ## Keyword search python Exporter.py --querysearch &quot;impeach&quot; --maxtweets 1000 --output impeach.csv ## username search with time period and size limit python Exporter.py --username &quot;realDonaldTrump&quot; --maxtweets 100 --output dt_100.csv For Python 3.x, replace Exporter.py with Exporter_py3.py "],
["Datavisualization.html", "Chapter 4 Data Visualization 4.1 Principles of Programming 4.2 Functionalities of Data Programs 4.3 Data programming in R 4.4 Data Visualization with R 4.5 Recommended R Resources:", " Chapter 4 Data Visualization This chapter starts with basic principles for data programming or coding involving data. Data programming is a practice that works and evolves with data. Data programming or coding allows the user to manage and process data in more effective manner. Programs are designed to be replicated or replicable by user and collaborators. A data program can be developed and updated iteratively and incrementally. In other words, it is building on the culminated works without repeating the steps. It takes debugging, which is the process of identifying problems (bugs) but, in fact, updating the program in different situations or with different inputs when used in different contexts, including the programmer himself or herself working in future times. 4.1 Principles of Programming Social scientists Gentzkow and Shapiro (2014) list out some principles for data programming. Automation For replicability (future-proof, for the future you) Version Control Allow evolution and updated edition Use Git and GitHub Directories/Modularity Organize by functions and data chunks Keys Index variable (relational) Abstraction KISS (Keep in short and simple) Documentation Comments for communicating to later users Management Collaboration ready 4.2 Functionalities of Data Programs A data program can provide or perform : Documentation of data Importing and exporting data Management of data Visualization of data Data models 4.3 Data programming in R R basics # Create variables composed of random numbers x &lt;-rnorm(50) y = rnorm(x) # Plot the points in the plane plot(x, y) Using R packages # Plot better, using the ggplot2 package ## Prerequisite: install and load the ggplot2 package ## install.packages(&quot;ggplot2&quot;) library(ggplot2) qplot(x,y) 4.4 Data Visualization with R # Plot better better with ggplot2 x &lt;-rnorm(50) y = rnorm(x) ggplot(,aes(x,y)) + theme_bw() + geom_point(col=&quot;blue&quot;) 4.4.1 Hands-on workshop I: Visualize Taiwan election data In this section, we demonstrate exploring data about Taiwan elections in 2016. The Taiwan Election and Democratization Study (TEDS) is one of the longest and most comprehensive elections studies starting in 2001. TEDS collects data through different modes of surveys including face-to-face interviews, telephone interviews and internet surveys. More detail of TEDS can be found at the National Chengchi University Election Study Center website at https://esc.nccu.edu.tw/main.php. Taiwan Election and Democratization Study 2016 data # Import the TEDS 2016 data in Stata format using the haven package ##install.packages(&quot;haven&quot;) library(haven) TEDS_2016 &lt;- haven::read_stata(&quot;https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true&quot;) # Prepare the analyze the Party ID variable # Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=&quot;NA&quot;) TEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(&quot;KMT&quot;,&quot;DPP&quot;,&quot;NP&quot;,&quot;PFP&quot;, &quot;TSU&quot;, &quot;NPP&quot;,&quot;NA&quot;)) Take a look at the variable: # Check the variable attach(TEDS_2016) head(PartyID) ## [1] NA NA KMT NA NA DPP ## Levels: KMT DPP NP PFP TSU NPP NA tail(PartyID) ## [1] NA NA DPP NA NA NA ## Levels: KMT DPP NP PFP TSU NPP NA Frequency table: # Run a frequency table of the Party ID variable using the descr package ## install.packages(&quot;descr&quot;) library(descr) freq(TEDS_2016$PartyID) ## TEDS_2016$PartyID ## Frequency Percent ## KMT 388 22.9586 ## DPP 591 34.9704 ## NP 3 0.1775 ## PFP 32 1.8935 ## TSU 5 0.2959 ## NPP 43 2.5444 ## NA 628 37.1598 ## Total 1690 100.0000 Get a better chart of the Party ID variable: # Plot the Party ID variable library(ggplot2) ggplot(TEDS_2016, aes(PartyID)) + geom_bar() We can attend to more detail of the chart, such as adding labels to x and y axes, and calculating the percentage instead of counts. ggplot2::ggplot(TEDS_2016, aes(PartyID)) + geom_bar(aes(y = (..count..)/sum(..count..))) + scale_y_continuous(labels=scales::percent) + ylab(&quot;Party Support (%)&quot;) + xlab(&quot;Taiwan Political Parties&quot;) Adding colors, with another theme: ggplot2::ggplot(TEDS_2016, aes(PartyID)) + geom_bar(aes(y = (..count..)/sum(..count..),fill=PartyID)) + scale_y_continuous(labels=scales::percent) + ylab(&quot;Party Support (%)&quot;) + xlab(&quot;Taiwan Political Parties&quot;) + theme_bw() Hold on, colors are not right! ggplot2::ggplot(TEDS_2016, aes(PartyID)) + geom_bar(aes(y = (..count..)/sum(..count..),fill=PartyID)) + scale_y_continuous(labels=scales::percent) + ylab(&quot;Party Support (%)&quot;) + xlab(&quot;Taiwan Political Parties&quot;) + theme_bw() + scale_fill_manual(values=c(&quot;steel blue&quot;,&quot;forestgreen&quot;,&quot;khaki1&quot;,&quot;orange&quot;,&quot;goldenrod&quot;,&quot;yellow&quot;,&quot;grey&quot;)) To make the chart more meaningful, we can use a package called tidyverse to manage the data. ##install.packages(&quot;tidyverse&quot;) library(tidyverse) TEDS_2016 %&gt;% count(PartyID) %&gt;% mutate(perc = n / nrow(TEDS_2016)) -&gt; T2 ggplot2::ggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + geom_bar(stat = &quot;identity&quot;) + ylab(&quot;Party Support (%)&quot;) + xlab(&quot;Taiwan Political Parties&quot;) + theme_bw() + scale_fill_manual(values=c(&quot;steel blue&quot;,&quot;forestgreen&quot;,&quot;khaki1&quot;,&quot;orange&quot;,&quot;goldenrod&quot;,&quot;yellow&quot;,&quot;grey&quot;)) 4.4.2 Hands-on workshop II: Visualize Hans Rosling’s famous gapminder chart In this section, we replicate Dr. Hans Rosling’s well-renowned animated chart depicting world development over time. For more detail, watch this BBC video. Data are drawn from Gapfinder, a foundation established by the Rosling family. my_packages &lt;- c(&quot;tidyverse&quot;, &quot;png&quot;,&quot;gifski&quot;, &quot;gapminder&quot;, &quot;ggplot2&quot;,&quot;gganimate&quot;,&quot;RColorBrewer&quot;) install.packages(my_packages, repos = &quot;http://cran.rstudio.com&quot;) ## ## The downloaded binary packages are in ## /var/folders/qp/s6y46pq11y13t0gpnf4_v9vm0000gp/T//RtmpbRF1QK/downloaded_packages library(gapminder) library(ggplot2) library(gganimate) library(gifski) library(png) library(RColorBrewer) data(&quot;gapminder&quot;) # Basic scatter plot object mapping &lt;- aes(x =gdpPercap, y = lifeExp, size = pop, color = continent, frame = year) # Note: manual color choices. ggplot(gapminder, mapping = mapping) + geom_point() + theme_linedraw() + scale_x_log10() + scale_color_manual(values=c(&quot;darkviolet&quot;,&quot;darkblue&quot;,&quot;firebrick1&quot;,&quot;forestgreen&quot;,&quot;deepskyblue1&quot;)) + labs(title = &#39;Year: {frame_time}&#39;, x = &#39;GDP per capita&#39;, y = &#39;life expectancy&#39;) + geom_text(aes(label=ifelse((country == &quot;China&quot;), &quot;China&quot;, ifelse(country==&quot;United States&quot;, &quot;United States&quot;, &quot;&quot;))),vjust=0,nudge_y = 1,size=6) + transition_time(year) + ease_aes(&#39;linear&#39;) ## Exercise # 1. Check the variables in gapminder # 2. Change the variable gdpPercap to size and pop to x # 3. Change the color palette using [RColorBrewer](https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf) 4.5 Recommended R Resources: The R Journal Introduction to R by W. N. Venables, D. M. Smith and the R Core Team Introduction to R Seminar at UCLA Getting Started in Data Analysis using Stata and R by Data and Statistical Services, Princeton University "]
]
